{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae2994dc-2979-43b8-ae86-eb40cf19ba62",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_network' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 117\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Load network\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mload_network\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnode_based_model.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    118\u001b[0m network \u001b[38;5;241m=\u001b[39m build_network(data)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Load MNIST dataset\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_network' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from kafka import KafkaProducer, KafkaConsumer, KafkaAdminClient\n",
    "from kafka.admin import NewTopic\n",
    "import redis\n",
    "import time\n",
    "\n",
    "# Redis setup\n",
    "redis_client = redis.Redis(host='host.docker.internal', port=6379, db=0)\n",
    "\n",
    "# Kafka Admin setup for topic management\n",
    "admin_client = KafkaAdminClient(bootstrap_servers='kafka:9092')\n",
    "\n",
    "def delete_messages(topic):\n",
    "    \"\"\"Deletes all messages in a topic by recreating it.\"\"\"\n",
    "    try:\n",
    "        admin_client.delete_topics([topic])  # Delete topic\n",
    "        time.sleep(2)  # Wait for deletion\n",
    "        admin_client.create_topics([NewTopic(name=topic, num_partitions=10, replication_factor=1)])  # Recreate topic\n",
    "        print(f\"üóëÔ∏è Deleted all messages in topic {topic} and recreated it.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not delete messages in topic {topic}: {e}\")\n",
    "\n",
    "# Activation functions\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))  # Stability trick\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "ACTIVATIONS = {\n",
    "    \"relu\": relu,\n",
    "    \"softmax\": softmax\n",
    "}\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, layer_id, neuron_id, weights, bias, activation, is_final_layer=False):\n",
    "        self.layer_id = layer_id\n",
    "        self.neuron_id = neuron_id\n",
    "        self.weights = np.array(weights)\n",
    "        self.bias = np.array(bias)\n",
    "        self.activation_func = None if is_final_layer else ACTIVATIONS.get(activation, relu)\n",
    "        self.is_final_layer = is_final_layer\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Consume messages from Kafka without using consumer groups.\"\"\"\n",
    "        topic = f'layer-{self.layer_id[-1]}'\n",
    "        consumer = KafkaConsumer(\n",
    "            topic,\n",
    "            bootstrap_servers='kafka:9092',\n",
    "            value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
    "            auto_offset_reset='earliest',  # ‚úÖ Always read messages from the beginning\n",
    "            enable_auto_commit=True\n",
    "        )\n",
    "\n",
    "        print(f\"üïí Neuron {self.neuron_id} in {self.layer_id} waiting for activation on topic {topic}...\")\n",
    "\n",
    "        for message in consumer:\n",
    "            print(f\"‚úÖ Neuron {self.neuron_id} received message: {message.value}\")\n",
    "            if 'layer' in message.value and message.value['layer'] == self.layer_id:\n",
    "                print(f\"üöÄ Neuron {self.neuron_id} in {self.layer_id} activated!\")\n",
    "                break\n",
    "\n",
    "        consumer.close()\n",
    "\n",
    "        z = np.dot(inputs, self.weights) + self.bias\n",
    "        return z if self.is_final_layer else self.activation_func(z)\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, layer_id, neurons, is_final_layer=False):\n",
    "        self.layer_id = layer_id\n",
    "        self.neurons = neurons\n",
    "        self.is_final_layer = is_final_layer\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        \"\"\"Trigger neuron activations via Kafka, activate the next layer, then delete messages.\"\"\"\n",
    "        producer = KafkaProducer(\n",
    "            bootstrap_servers='kafka:9092',\n",
    "            value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    "        )\n",
    "\n",
    "        topic = f'layer-{self.layer_id[-1]}'\n",
    "        activation_message = {'layer': self.layer_id}\n",
    "\n",
    "        print(f\"üì§ Layer {self.layer_id} sending activation messages to topic {topic}...\")\n",
    "\n",
    "        try:\n",
    "            producer.send(topic, value=activation_message)\n",
    "            producer.flush()\n",
    "            print(f\"‚úÖ Kafka message sent to {topic}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Kafka send failed: {e}\")\n",
    "\n",
    "        producer.close()\n",
    "\n",
    "        outputs = np.array([neuron.forward(input_data) for neuron in self.neurons])\n",
    "        if self.is_final_layer:\n",
    "            outputs = softmax(outputs)\n",
    "\n",
    "        redis_client.set(self.layer_id, outputs.astype(np.float32).tobytes())\n",
    "\n",
    "        # ‚úÖ Activate the next layer via \"activate-layer\" topic before deleting messages\n",
    "        producer = KafkaProducer(bootstrap_servers='kafka:9092',\n",
    "                                 value_serializer=lambda v: json.dumps(v).encode('utf-8'))\n",
    "        producer.send('activate-layer', {'layer': f'layer-{int(self.layer_id[-1]) + 1}'} if not self.is_final_layer else {'layer': 'final'})\n",
    "        producer.flush()\n",
    "        producer.close()\n",
    "\n",
    "        # ‚úÖ Delete all messages in the layer's topic AFTER activating the next layer\n",
    "        delete_messages(topic)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "# Load network\n",
    "data = load_network(\"node_based_model.json\")\n",
    "network = build_network(data)\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "mnist_test = datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "# Test first 10 images one by one\n",
    "for i in range(10):\n",
    "    image, label = mnist_test[i]\n",
    "    image_np = image.view(-1).numpy()\n",
    "    prediction = forward_pass(network, image_np, i)\n",
    "    print(f\"Image {i} Prediction: {prediction}, Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc4d8c2-fbe2-418e-9b4f-3682a00884c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
