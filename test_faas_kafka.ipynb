{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba037847-765f-4f9f-8a72-14e75e92c8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 0 Prediction: 7, Label: 7\n",
      "Image 1 Prediction: 2, Label: 2\n",
      "Image 2 Prediction: 1, Label: 1\n",
      "Image 3 Prediction: 0, Label: 0\n",
      "Image 4 Prediction: 4, Label: 4\n",
      "Image 5 Prediction: 1, Label: 1\n",
      "Image 6 Prediction: 4, Label: 4\n",
      "Image 7 Prediction: 9, Label: 9\n",
      "Image 8 Prediction: 6, Label: 5\n",
      "Image 9 Prediction: 9, Label: 9\n",
      "Test Accuracy: 90.00%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "import redis\n",
    "import time\n",
    "\n",
    "# Redis setup\n",
    "redis_client = redis.Redis(host='host.docker.internal', port=6379, db=0)\n",
    "\n",
    "# Kafka setup\n",
    "producer = KafkaProducer(bootstrap_servers='kafka:29092',\n",
    "                         value_serializer=lambda v: json.dumps(v).encode('utf-8'))\n",
    "consumer = KafkaConsumer('activate-layer', bootstrap_servers='kafka:29092',\n",
    "                         value_deserializer=lambda m: json.loads(m.decode('utf-8')))\n",
    "\n",
    "# Load JSON file\n",
    "def load_network(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Activation functions\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))  # Stability trick\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "ACTIVATIONS = {\n",
    "    \"relu\": relu,\n",
    "    \"softmax\": softmax\n",
    "}\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, weights, bias, activation, is_final_layer=False):\n",
    "        self.weights = np.array(weights)\n",
    "        self.bias = np.array(bias)\n",
    "        self.activation_func = None if is_final_layer else ACTIVATIONS.get(activation, relu)\n",
    "        self.is_final_layer = is_final_layer\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Compute neuron output\"\"\"\n",
    "        z = np.dot(inputs, self.weights) + self.bias  # (num_features,)\n",
    "        return z if self.is_final_layer else self.activation_func(z)\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, layer_id, neurons, is_final_layer=False):\n",
    "        self.layer_id = layer_id\n",
    "        self.neurons = neurons\n",
    "        self.is_final_layer = is_final_layer\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        \"\"\"Process one input sample at a time\"\"\"\n",
    "        outputs = np.array([neuron.forward(input_data) for neuron in self.neurons])\n",
    "        if self.is_final_layer:\n",
    "            outputs = softmax(outputs)\n",
    "        redis_client.set(self.layer_id, outputs.astype(np.float32).tobytes())\n",
    "        producer.send('activate-layer', {'layer': self.layer_id})\n",
    "        return outputs\n",
    "\n",
    "# Build network\n",
    "def build_network(json_data):\n",
    "    layers = []\n",
    "    sorted_layers = sorted(json_data.keys(), key=lambda x: int(x.split('_')[-1]))\n",
    "    for i, layer_name in enumerate(sorted_layers):\n",
    "        layer_info = json_data[layer_name]\n",
    "        neurons = [\n",
    "            Neuron(\n",
    "                weights=np.array(node['weights']),\n",
    "                bias=np.array(node['biases']),\n",
    "                activation=node['activation'],\n",
    "                is_final_layer=(i == len(sorted_layers) - 1)\n",
    "            )\n",
    "            for node in layer_info['nodes']\n",
    "        ]\n",
    "        layers.append(Layer(layer_id=layer_name, neurons=neurons, is_final_layer=(i == len(sorted_layers) - 1)))\n",
    "    return layers\n",
    "\n",
    "# Forward pass for single image\n",
    "def forward_pass(layers, input_data):\n",
    "    for layer in layers:\n",
    "        input_data = layer.forward(input_data)\n",
    "    redis_client.set('results', json.dumps(int(np.argmax(input_data))))\n",
    "    producer.send('activate-layer', {'layer': 'final'})\n",
    "\n",
    "# Load network\n",
    "data = load_network(\"node_based_model.json\")\n",
    "network = build_network(data)\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "])\n",
    "mnist_test = datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "# Test first 10 images one by one\n",
    "correct_predictions = 0\n",
    "for i in range(10):\n",
    "    image, label = mnist_test[i]\n",
    "    image_np = image.view(-1).numpy()\n",
    "    redis_client.delete('results')\n",
    "    forward_pass(network, image_np)\n",
    "\n",
    "    timeout = 10  # seconds\n",
    "    waited = 0\n",
    "    while redis_client.get('results') is None and waited < timeout:\n",
    "        time.sleep(0.1)\n",
    "        waited += 0.1\n",
    "\n",
    "    prediction_json = redis_client.get('results')\n",
    "    if prediction_json:\n",
    "        prediction = json.loads(prediction_json)\n",
    "        print(f\"Image {i} Prediction: {prediction}, Label: {label}\")\n",
    "        if prediction == label:\n",
    "            correct_predictions += 1\n",
    "    else:\n",
    "        print(f\"No prediction received for image {i} within timeout.\")\n",
    "\n",
    "accuracy = correct_predictions / 10\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d59d199b-6760-42b9-b479-52c8f3c57f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 Sending initial activation message to layer-0...\n",
      "📤 Layer layer_0 sending activation message to topic layer-0...\n",
      "✅ Kafka message sent to layer-0: RecordMetadata(topic='layer-0', partition=114, topic_partition=TopicPartition(topic='layer-0', partition=114), offset=0, timestamp=1741375356999, log_start_offset=0, checksum=None, serialized_key_size=-1, serialized_value_size=20, serialized_header_size=-1)\n",
      "🕒 Neuron 0 in layer_0 waiting for activation on topic layer-0...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 141\u001b[0m\n\u001b[1;32m    139\u001b[0m image_np \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    140\u001b[0m redis_client\u001b[38;5;241m.\u001b[39mdelete(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 141\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mforward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_np\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Prediction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprediction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Label: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prediction \u001b[38;5;241m==\u001b[39m label:\n",
      "Cell \u001b[0;32mIn[2], line 120\u001b[0m, in \u001b[0;36mforward_pass\u001b[0;34m(layers, input_data)\u001b[0m\n\u001b[1;32m    117\u001b[0m producer\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m layers:\n\u001b[0;32m--> 120\u001b[0m     input_data \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m redis_client\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m'\u001b[39m, json\u001b[38;5;241m.\u001b[39mdumps(\u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39margmax(input_data))))\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39margmax(input_data))\n",
      "Cell \u001b[0;32mIn[2], line 76\u001b[0m, in \u001b[0;36mLayer.forward\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m     73\u001b[0m producer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m     74\u001b[0m producer\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m---> 76\u001b[0m outputs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([neuron\u001b[38;5;241m.\u001b[39mforward(input_data) \u001b[38;5;28;01mfor\u001b[39;00m neuron \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneurons])\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_final_layer:\n\u001b[1;32m     78\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m softmax(outputs)\n",
      "Cell \u001b[0;32mIn[2], line 76\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     73\u001b[0m producer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m     74\u001b[0m producer\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m---> 76\u001b[0m outputs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mneuron\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m neuron \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneurons])\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_final_layer:\n\u001b[1;32m     78\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m softmax(outputs)\n",
      "Cell \u001b[0;32mIn[2], line 40\u001b[0m, in \u001b[0;36mNeuron.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     36\u001b[0m consumer \u001b[38;5;241m=\u001b[39m KafkaConsumer(topic, bootstrap_servers\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkafka:9092\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     37\u001b[0m                          value_deserializer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m m: json\u001b[38;5;241m.\u001b[39mloads(m\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)), auto_offset_reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatest\u001b[39m\u001b[38;5;124m'\u001b[39m,enable_auto_commit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🕒 Neuron \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneuron_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m waiting for activation on topic \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtopic\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m consumer:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Neuron \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneuron_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m activated with message: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m message\u001b[38;5;241m.\u001b[39mvalue[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_id:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/kafka/consumer/group.py:1197\u001b[0m, in \u001b[0;36mKafkaConsumer.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_v1()\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/kafka/consumer/group.py:1205\u001b[0m, in \u001b[0;36mKafkaConsumer.next_v2\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_message_generator_v2()\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   1207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/kafka/consumer/group.py:1120\u001b[0m, in \u001b[0;36mKafkaConsumer._message_generator_v2\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_message_generator_v2\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1119\u001b[0m     timeout_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consumer_timeout \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mtime())\n\u001b[0;32m-> 1120\u001b[0m     record_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout_ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_offsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tp, records \u001b[38;5;129;01min\u001b[39;00m six\u001b[38;5;241m.\u001b[39miteritems(record_map):\n\u001b[1;32m   1122\u001b[0m         \u001b[38;5;66;03m# Generators are stateful, and it is possible that the tp / records\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m         \u001b[38;5;66;03m# here may become stale during iteration -- i.e., we seek to a\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m         \u001b[38;5;66;03m# different offset, pause consumption, or lose assignment.\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m records:\n\u001b[1;32m   1126\u001b[0m             \u001b[38;5;66;03m# is_fetchable(tp) should handle assignment changes and offset\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m             \u001b[38;5;66;03m# resets; for all other changes (e.g., seeks) we'll rely on the\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m             \u001b[38;5;66;03m# outer function destroying the existing iterator/generator\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m             \u001b[38;5;66;03m# via self._iterator = None\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/kafka/consumer/group.py:657\u001b[0m, in \u001b[0;36mKafkaConsumer.poll\u001b[0;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[1;32m    655\u001b[0m remaining \u001b[38;5;241m=\u001b[39m timeout_ms\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_closed:\n\u001b[0;32m--> 657\u001b[0m     records \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_records\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_offsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupdate_offsets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m records:\n\u001b[1;32m    659\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m records\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/kafka/consumer/group.py:706\u001b[0m, in \u001b[0;36mKafkaConsumer._poll_once\u001b[0;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mpoll(timeout_ms\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    705\u001b[0m timeout_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(timeout_ms, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coordinator\u001b[38;5;241m.\u001b[39mtime_to_next_poll() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m--> 706\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout_ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;66;03m# after the long poll, we should check whether the group needs to rebalance\u001b[39;00m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;66;03m# prior to returning data so that the group can stabilize faster\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coordinator\u001b[38;5;241m.\u001b[39mneed_rejoin():\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/kafka/client_async.py:614\u001b[0m, in \u001b[0;36mKafkaClient.poll\u001b[0;34m(self, timeout_ms, future)\u001b[0m\n\u001b[1;32m    607\u001b[0m         timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\n\u001b[1;32m    608\u001b[0m             timeout_ms,\n\u001b[1;32m    609\u001b[0m             metadata_timeout_ms,\n\u001b[1;32m    610\u001b[0m             idle_connection_timeout_ms,\n\u001b[1;32m    611\u001b[0m             request_timeout_ms)\n\u001b[1;32m    612\u001b[0m         timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, timeout)  \u001b[38;5;66;03m# avoid negative timeouts\u001b[39;00m\n\u001b[0;32m--> 614\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;66;03m# called without the lock to avoid deadlock potential\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;66;03m# if handlers need to acquire locks\u001b[39;00m\n\u001b[1;32m    618\u001b[0m responses\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fire_pending_completed_requests())\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/kafka/client_async.py:648\u001b[0m, in \u001b[0;36mKafkaClient._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_register_send_sockets()\n\u001b[1;32m    647\u001b[0m start_select \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 648\u001b[0m ready \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    649\u001b[0m end_select \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sensors:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/selectors.py:469\u001b[0m, in \u001b[0;36mEpollSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    467\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 469\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ev\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "import redis\n",
    "import time\n",
    "\n",
    "# Redis setup\n",
    "redis_client = redis.Redis(host='host.docker.internal', port=6379, db=0)\n",
    "\n",
    "# Activation functions\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))  # Stability trick\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "ACTIVATIONS = {\n",
    "    \"relu\": relu,\n",
    "    \"softmax\": softmax\n",
    "}\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, layer_id, neuron_id, weights, bias, activation, is_final_layer=False):\n",
    "        self.layer_id = layer_id\n",
    "        self.neuron_id = neuron_id\n",
    "        self.weights = np.array(weights)\n",
    "        self.bias = np.array(bias)\n",
    "        self.activation_func = None if is_final_layer else ACTIVATIONS.get(activation, relu)\n",
    "        self.is_final_layer = is_final_layer\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Wait for activation message from Kafka before processing\"\"\"\n",
    "        topic = f'layer-{self.layer_id[-1]}'\n",
    "        consumer = KafkaConsumer(topic, bootstrap_servers='kafka:9092',\n",
    "                                 value_deserializer=lambda m: json.loads(m.decode('utf-8')), auto_offset_reset='latest',enable_auto_commit=True)\n",
    "        print(f\"🕒 Neuron {self.neuron_id} in {self.layer_id} waiting for activation on topic {topic}...\")\n",
    "\n",
    "        for message in consumer:\n",
    "            print(f\"✅ Neuron {self.neuron_id} activated with message: {message.value}\")\n",
    "            if message.value['layer'] == self.layer_id:\n",
    "                break  # Process once activation message is received\n",
    "        consumer.close()\n",
    "\n",
    "        z = np.dot(inputs, self.weights) + self.bias\n",
    "        return z if self.is_final_layer else self.activation_func(z)\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, layer_id, neurons, is_final_layer=False):\n",
    "        self.layer_id = layer_id\n",
    "        self.neurons = neurons\n",
    "        self.is_final_layer = is_final_layer\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        \"\"\"Trigger neuron activations via Kafka\"\"\"\n",
    "        producer = KafkaProducer(bootstrap_servers='kafka:9092',\n",
    "                                 value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
    "                                 retries=5, request_timeout_ms=10000)\n",
    "    \n",
    "        topic = f'layer-{self.layer_id[-1]}'  # Ensure topic follows `layer-0`, `layer-1` format\n",
    "        activation_message = {'layer': self.layer_id}\n",
    "    \n",
    "        print(f\"📤 Layer {self.layer_id} sending activation message to topic {topic}...\")\n",
    "    \n",
    "        try:\n",
    "            future = producer.send(topic, activation_message)\n",
    "            result = future.get(timeout=10)  # Wait for response\n",
    "            print(f\"✅ Kafka message sent to {topic}: {result}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Kafka send failed: {e}\")\n",
    "    \n",
    "        producer.flush()\n",
    "        producer.close()\n",
    "    \n",
    "        outputs = np.array([neuron.forward(input_data) for neuron in self.neurons])\n",
    "        if self.is_final_layer:\n",
    "            outputs = softmax(outputs)\n",
    "    \n",
    "        redis_client.set(self.layer_id, outputs.astype(np.float32).tobytes())\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# Load JSON file\n",
    "def load_network(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Build network\n",
    "def build_network(json_data):\n",
    "    layers = []\n",
    "    sorted_layers = sorted(json_data.keys(), key=lambda x: int(x.split('_')[-1]))\n",
    "    for i, layer_name in enumerate(sorted_layers):\n",
    "        layer_info = json_data[layer_name]\n",
    "        neurons = [\n",
    "            Neuron(\n",
    "                layer_id=layer_name,\n",
    "                neuron_id=idx,\n",
    "                weights=np.array(node['weights']),\n",
    "                bias=np.array(node['biases']),\n",
    "                activation=node['activation'],\n",
    "                is_final_layer=(i == len(sorted_layers) - 1)\n",
    "            )\n",
    "            for idx, node in enumerate(layer_info['nodes'])\n",
    "        ]\n",
    "        layers.append(Layer(layer_id=layer_name, neurons=neurons, is_final_layer=(i == len(sorted_layers) - 1)))\n",
    "    return layers\n",
    "\n",
    "# Forward pass for single image\n",
    "def forward_pass(layers, input_data):\n",
    "    producer = KafkaProducer(bootstrap_servers='kafka:9092',\n",
    "                             value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
    "                            )\n",
    "    print(\"🔥 Sending initial activation message to layer-0...\")\n",
    "    producer.send('layer-0', {'layer': 'layer-0'})\n",
    "    producer.flush()\n",
    "    producer.close()\n",
    "    \n",
    "    for layer in layers:\n",
    "        input_data = layer.forward(input_data)\n",
    "    redis_client.set('results', json.dumps(int(np.argmax(input_data))))\n",
    "    return int(np.argmax(input_data))\n",
    "\n",
    "# Load network\n",
    "data = load_network(\"node_based_model.json\")\n",
    "network = build_network(data)\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "])\n",
    "mnist_test = datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "# Test first 10 images one by one\n",
    "correct_predictions = 0\n",
    "for i in range(10):\n",
    "    image, label = mnist_test[i]\n",
    "    image_np = image.view(-1).numpy()\n",
    "    redis_client.delete('results')\n",
    "    prediction = forward_pass(network, image_np)\n",
    "\n",
    "    print(f\"Image {i} Prediction: {prediction}, Label: {label}\")\n",
    "    if prediction == label:\n",
    "        correct_predictions += 1\n",
    "\n",
    "accuracy = correct_predictions / 10\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0465401-d2c7-4fc0-8c95-eeb98a0febff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Kafka message sent to 'activate-layer': {'layer': 'layer_0'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "import redis\n",
    "import time\n",
    "\n",
    "# Redis setup\n",
    "redis_client = redis.Redis(host='host.docker.internal', port=6379, db=0)\n",
    "\n",
    "# Kafka setup\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers='kafka:29092',\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    'activate-layer',\n",
    "    bootstrap_servers='kafka:29092',\n",
    "    value_deserializer=lambda m: json.loads(m.decode('utf-8'))\n",
    ")\n",
    "\n",
    "try:\n",
    "    activation_message = {'layer': \"layer_0\"}\n",
    "    future = producer.send('activate-layer', activation_message)  # ✅ FIXED\n",
    "    producer.flush()\n",
    "    print(f\"✅ Kafka message sent to 'activate-layer': {activation_message}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Kafka send failed: {e}\")\n",
    "finally:\n",
    "    producer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc84c63-fa86-471f-875e-2243139d74a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
