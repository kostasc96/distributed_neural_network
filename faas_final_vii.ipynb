{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa81c0b4-3c35-4e4f-bd73-e2e060a0a55a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import redis \n",
    "\n",
    "client = redis.Redis('host.docker.internal', 6379, 0)\n",
    "\n",
    "client.flushdb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab027b81-0c09-4f3b-a568-8e84fbaaa179",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "beeba56c-9bd8-49f4-acca-638f4acbf4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import threading\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pcomp.kafka_handlers import KafkaProducerHandler, KafkaConsumerHandler, KafkaConsumerHandlerNeuron\n",
    "from pcomp.activation_functions import ACTIVATIONS, relu, softmax\n",
    "from pcomp.redis_utils import RedisHandler\n",
    "from pcomp.parser import parse_layer_coordinator_message, parse_layer_message\n",
    "from pcomp.dotmodule import dot_with_bias\n",
    "from pcomp.neurons_accumulator import NeuronsAccumulator\n",
    "from pcomp.s3client import S3Client\n",
    "from io import StringIO\n",
    "\n",
    "# Kafka Configuration\n",
    "KAFKA_BROKER = 'kafka:29092'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98839d08-ace4-435b-9a31-44a666f36399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threads started\n",
      "ðŸš€ Uploading predictions/batch_00000.csv (1110 bytes) to S3...\n",
      "âœ… Uploaded predictions/batch_00000.csv to bucket my-bucket\n",
      "ðŸš€ Uploading predictions/batch_00001.csv (1220 bytes) to S3...\n",
      "âœ… Uploaded predictions/batch_00001.csv to bucket my-bucket\n",
      "ðŸš€ Uploading predictions/batch_00002.csv (1220 bytes) to S3...\n",
      "âœ… Uploaded predictions/batch_00002.csv to bucket my-bucket\n",
      "ðŸš€ Uploading predictions/batch_00003.csv (1220 bytes) to S3...\n",
      "âœ… Uploaded predictions/batch_00003.csv to bucket my-bucket\n",
      "ðŸš€ Uploading predictions/batch_00004.csv (1220 bytes) to S3...\n",
      "âœ… Uploaded predictions/batch_00004.csv to bucket my-bucket\n"
     ]
    }
   ],
   "source": [
    "class Neuron(threading.Thread):\n",
    "    def __init__(self, layer_id, neuron_id, weights, bias, activation, is_final_layer):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.layer_id = layer_id\n",
    "        self.layer_id_num = int(self.layer_id.replace(\"layer_\", \"\"))\n",
    "        self.neuron_id = neuron_id\n",
    "        self.weights = np.array(weights)\n",
    "        self.bias = np.array(bias)\n",
    "        self.is_final_layer = is_final_layer\n",
    "        self.activation = None if self.is_final_layer else activation\n",
    "        self.activation_func = None if self.is_final_layer else ACTIVATIONS.get(activation, relu)\n",
    "        self.redis_handler = RedisHandler('host.docker.internal', 6379, 0, 3)\n",
    "        self.producer = None\n",
    "        \n",
    "\n",
    "    def fetch_input(self, image_id):\n",
    "        key = f\"initial_data_{image_id}\" if self.layer_id_num == 0 else f\"{self.layer_id_num - 1}_{image_id}\"\n",
    "        return np.frombuffer(self.redis_handler.get(key), dtype=np.float64)\n",
    "\n",
    "    def process_and_send(self, image_id, input_data):\n",
    "        #output = dot_with_bias(input_data, self.weights, self.bias, self.activation, False)\n",
    "        z = np.dot(input_data, self.weights) + self.bias\n",
    "        output = z if self.is_final_layer else self.activation_func(z)\n",
    "        self.producer.send_with_key(str(image_id), f\"{self.neuron_id}|{format(output, '.17g')}\")\n",
    "\n",
    "    def run(self):\n",
    "        # Instantiate Kafka consumer and producer inside the thread.\n",
    "        consumer_handler = KafkaConsumerHandler(f'layer-{self.layer_id_num}', KAFKA_BROKER, group_id=f\"{self.neuron_id}_{self.layer_id_num}_group\")\n",
    "        consumer = consumer_handler.get_consumer()\n",
    "        self.producer = KafkaProducerHandler(KAFKA_BROKER, f'layer-{self.layer_id_num}-streams')\n",
    "        last_msg_time = time.time()\n",
    "        while True:\n",
    "            got_message = False\n",
    "            for msg in consumer.consume(500, timeout=0.2):\n",
    "                if msg.error():\n",
    "                    consumer_handler.error_handling(msg)\n",
    "                message = msg.value().decode('utf-8')\n",
    "                got_message = True\n",
    "                last_msg_time = time.time()\n",
    "                image_id_str = message\n",
    "                image_id = int(image_id_str)\n",
    "                try:\n",
    "                    input_data = self.fetch_input(image_id)\n",
    "                    self.process_and_send(image_id, input_data)\n",
    "                except Exception:\n",
    "                    pass\n",
    "            if not got_message and (time.time() - last_msg_time > 10):\n",
    "                consumer.commit()\n",
    "                consumer.close()\n",
    "                self.producer.close()\n",
    "                self.redis_handler.close()\n",
    "                break\n",
    "\n",
    "\n",
    "import threading\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from io import BytesIO\n",
    "\n",
    "class NeuronOutput(threading.Thread):\n",
    "    def __init__(\n",
    "        self,\n",
    "        s3_client=None,\n",
    "        bucket_name=\"my-bucket\",\n",
    "        prefix=\"predictions\",\n",
    "        batch_size=200,\n",
    "        batch_timeout=60  # seconds\n",
    "    ):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.s3_client = s3_client or S3Client(\"host.docker.internal:9000\", \"admin\", \"admin123\")\n",
    "        self.executor = ThreadPoolExecutor(max_workers=4)\n",
    "        self.bucket_name = bucket_name\n",
    "        self.prefix = prefix\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_timeout = batch_timeout\n",
    "        self.file_counter = 0\n",
    "        self.lock = threading.Lock()\n",
    "        # Initialize batching state\n",
    "        self._reset_batch()\n",
    "\n",
    "    def _reset_batch(self):\n",
    "        self.csv_buffer = BytesIO()\n",
    "        self.csv_buffer.write(b\"image_id,prediction\\n\")  # CSV header directly as bytes\n",
    "        self.record_count = 0\n",
    "        self.last_batch_time = time.time()\n",
    "\n",
    "    def run(self):\n",
    "        consumer_handler = KafkaConsumerHandler('layer-output', KAFKA_BROKER, group_id=\"neuron_output_coord_group\")\n",
    "        consumer = consumer_handler.get_consumer()\n",
    "        last_msg_time = time.time()\n",
    "\n",
    "        while True:\n",
    "            got_message = False\n",
    "            for msg in consumer.consume(500, timeout=0.2):\n",
    "                if msg.error():\n",
    "                    consumer_handler.error_handling(msg)\n",
    "                    continue\n",
    "\n",
    "                image_id, prediction = msg.value().decode('utf-8').split(\"|\")\n",
    "                got_message = True\n",
    "                last_msg_time = time.time()\n",
    "\n",
    "                with self.lock:\n",
    "                    line = f\"{image_id},{prediction}\\n\".encode(\"utf-8\")\n",
    "                    self.csv_buffer.write(line)\n",
    "                    self.record_count += 1\n",
    "                    should_flush = (\n",
    "                        self.record_count >= self.batch_size or\n",
    "                        (time.time() - self.last_batch_time > self.batch_timeout)\n",
    "                    )\n",
    "\n",
    "                if should_flush:\n",
    "                    self.flush_batch_async()\n",
    "\n",
    "            if not got_message and (time.time() - last_msg_time > 15):\n",
    "                consumer.commit()\n",
    "                consumer.close()\n",
    "                with self.lock:\n",
    "                    if self.record_count > 0:\n",
    "                        self.flush_batch_async()  # Flush remaining on shutdown\n",
    "                break\n",
    "\n",
    "    def flush_batch_async(self):\n",
    "        with self.lock:\n",
    "            if self.record_count == 0:\n",
    "                return\n",
    "\n",
    "            # Snapshot the buffer reference (BytesIO object)\n",
    "            buffer_snapshot = self.csv_buffer\n",
    "            file_name_snapshot = f\"{self.prefix}/batch_{self.file_counter:05d}.csv\"\n",
    "            self.file_counter += 1\n",
    "\n",
    "            # Reset buffer and counters immediately\n",
    "            self._reset_batch()\n",
    "\n",
    "        # Submit background flush\n",
    "        self.executor.submit(self.flush_batch_to_s3, buffer_snapshot, file_name_snapshot)\n",
    "\n",
    "    def flush_batch_to_s3(self, buffer, file_name):\n",
    "        try:\n",
    "            buffer.seek(0)  # Rewind buffer to start before reading\n",
    "            content = buffer.read()\n",
    "            print(f\"ðŸš€ Uploading {file_name} ({len(content)} bytes) to S3...\")\n",
    "            self.s3_client.put_object(self.bucket_name, file_name, content)\n",
    "            print(f\"âœ… Uploaded {file_name} to bucket {self.bucket_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error uploading {file_name} to S3: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# class NeuronOutput(threading.Thread):\n",
    "#     def __init__(self):\n",
    "#         threading.Thread.__init__(self)\n",
    "#         # self.redis_handler = RedisHandler('host.docker.internal', 6379, 0, 8)\n",
    "#         self.last_layer_id_num = 1\n",
    "#         self.executor = ThreadPoolExecutor(max_workers=4)\n",
    "\n",
    "#     def run(self):\n",
    "#         consumer_handler = KafkaConsumerHandler(f'layer-output', KAFKA_BROKER, group_id=f\"neuron_output_coord_group\")\n",
    "#         consumer = consumer_handler.get_consumer()\n",
    "#         last_msg_time = time.time()\n",
    "#         while True:\n",
    "#             got_message = False\n",
    "#             for msg in consumer.consume(500, timeout=0.2):\n",
    "#                 if msg.error():\n",
    "#                     consumer_handler.error_handling(msg)\n",
    "#                 message = msg.value().decode('utf-8')\n",
    "#                 got_message = True\n",
    "#                 last_msg_time = time.time()\n",
    "#                 image_id = int(message)\n",
    "#                 try:\n",
    "#                     key = f\"{self.last_layer_id_num}_{image_id}\"\n",
    "#                     #outputs = np.frombuffer(self.redis_handler.get(key), dtype=np.float64)\n",
    "#                     #prediction = int(np.argmax(outputs))\n",
    "#                     #self.redis_handler.hset('streams:predictions', image_id, prediction)\n",
    "#                 except Exception:\n",
    "#                     pass\n",
    "#             if not got_message and (time.time() - last_msg_time > 15):\n",
    "#                 consumer.commit()\n",
    "#                 consumer.close()\n",
    "#                 # self.redis_handler.close()\n",
    "#                 break\n",
    "\n",
    "# Load network and dataset\n",
    "data = json.load(open(\"node_based_model.json\"))\n",
    "\n",
    "neurons = []\n",
    "\n",
    "for layer_name, layer_info in data.items():\n",
    "    neurons += [Neuron(layer_id=layer_name, neuron_id=i, weights=node['weights'], bias=node['biases'], activation=node['activation'], is_final_layer=(layer_name == list(data.keys())[-1])) for i, node in enumerate(layer_info['nodes'])]\n",
    "\n",
    "neuron_output = NeuronOutput()\n",
    "\n",
    "# Start all threads\n",
    "for thread in neurons:\n",
    "    thread.start()\n",
    "\n",
    "neuron_output.start()\n",
    "\n",
    "print(\"Threads started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1b1a50c-ab95-481e-8073-37427cc70037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 95.30% (1906/2000)\n"
     ]
    }
   ],
   "source": [
    "import redis\n",
    "\n",
    "# Connect to Redis\n",
    "r = RedisHandler('host.docker.internal', 6379, 0)\n",
    "\n",
    "# Get hashes from Redis\n",
    "images_label = r.hgetall('images_label')\n",
    "predictions = r.hgetall('streams:predictions')\n",
    "\n",
    "# Decode bytes to string\n",
    "images_label = {k.decode(): v.decode() for k, v in images_label.items()}\n",
    "predictions = {k.decode(): v.decode() for k, v in predictions.items()}\n",
    "\n",
    "# Calculate accuracy\n",
    "correct = 0\n",
    "total = len(images_label)\n",
    "\n",
    "for field, label_val in images_label.items():\n",
    "    pred_val = predictions.get(field, None)\n",
    "    if pred_val == label_val:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}% ({correct}/{total})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8bb92a4-f4c2-46bb-bb4d-2742391fd88f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m thread \u001b[38;5;129;01min\u001b[39;00m neurons:\n\u001b[1;32m      3\u001b[0m     thread\u001b[38;5;241m.\u001b[39mjoin()\n\u001b[0;32m----> 5\u001b[0m \u001b[43mneuron_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThreads finished\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/threading.py:1060\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1057\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1060\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/threading.py:1080\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1077\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1080\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1081\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%4|1745628997.770|MAXPOLL|rdkafka#consumer-831| [thrd:main]: Application maximum poll interval (300000ms) exceeded by 64ms (adjust max.poll.interval.ms for long-running message processing): leaving group\n"
     ]
    }
   ],
   "source": [
    "# Wait for all threads to complete\n",
    "for thread in neurons:\n",
    "    thread.join()\n",
    "\n",
    "neuron_output.join()\n",
    "\n",
    "print(\"Threads finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95ff403-a113-44c7-940f-1f8ad0b391ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
