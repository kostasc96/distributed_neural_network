{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bda36f58-9f59-48b9-b380-91fb55762d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• Sending initial activation message to layer-0...\n",
      "üì§ Layer layer_0 sending activation messages to topic layer-0...\n",
      "‚úÖ Message sent to layer-0, partition 0\n",
      "‚úÖ Message sent to layer-0, partition 1\n",
      "‚úÖ Message sent to layer-0, partition 2\n",
      "‚úÖ Message sent to layer-0, partition 3\n",
      "‚úÖ Message sent to layer-0, partition 4\n",
      "‚úÖ Message sent to layer-0, partition 5\n",
      "‚úÖ Message sent to layer-0, partition 6\n",
      "‚úÖ Message sent to layer-0, partition 7\n",
      "‚úÖ Message sent to layer-0, partition 8\n",
      "‚úÖ Message sent to layer-0, partition 9\n",
      "‚úÖ Message sent to layer-0, partition 0\n",
      "‚úÖ Message sent to layer-0, partition 1\n",
      "‚úÖ Message sent to layer-0, partition 2\n",
      "‚úÖ Message sent to layer-0, partition 3\n",
      "‚úÖ Message sent to layer-0, partition 4\n",
      "‚úÖ Message sent to layer-0, partition 5\n",
      "‚úÖ Message sent to layer-0, partition 6\n",
      "‚úÖ Message sent to layer-0, partition 7\n",
      "‚úÖ Message sent to layer-0, partition 8\n",
      "‚úÖ Message sent to layer-0, partition 9\n",
      "‚úÖ Message sent to layer-0, partition 0\n",
      "‚úÖ Message sent to layer-0, partition 1\n",
      "‚úÖ Message sent to layer-0, partition 2\n",
      "‚úÖ Message sent to layer-0, partition 3\n",
      "‚úÖ Message sent to layer-0, partition 4\n",
      "‚úÖ Message sent to layer-0, partition 5\n",
      "‚úÖ Message sent to layer-0, partition 6\n",
      "‚úÖ Message sent to layer-0, partition 7\n",
      "‚úÖ Message sent to layer-0, partition 8\n",
      "‚úÖ Message sent to layer-0, partition 9\n",
      "‚úÖ Message sent to layer-0, partition 0\n",
      "‚úÖ Message sent to layer-0, partition 1\n",
      "‚úÖ Message sent to layer-0, partition 2\n",
      "‚úÖ Message sent to layer-0, partition 3\n",
      "‚úÖ Message sent to layer-0, partition 4\n",
      "‚úÖ Message sent to layer-0, partition 5\n",
      "‚úÖ Message sent to layer-0, partition 6\n",
      "‚úÖ Message sent to layer-0, partition 7\n",
      "‚úÖ Message sent to layer-0, partition 8\n",
      "‚úÖ Message sent to layer-0, partition 9\n",
      "‚úÖ Message sent to layer-0, partition 0\n",
      "‚úÖ Message sent to layer-0, partition 1\n",
      "‚úÖ Message sent to layer-0, partition 2\n",
      "‚úÖ Message sent to layer-0, partition 3\n",
      "‚úÖ Message sent to layer-0, partition 4\n",
      "‚úÖ Message sent to layer-0, partition 5\n",
      "‚úÖ Message sent to layer-0, partition 6\n",
      "‚úÖ Message sent to layer-0, partition 7\n",
      "‚úÖ Message sent to layer-0, partition 8\n",
      "‚úÖ Message sent to layer-0, partition 9\n",
      "‚úÖ Message sent to layer-0, partition 0\n",
      "‚úÖ Message sent to layer-0, partition 1\n",
      "‚úÖ Message sent to layer-0, partition 2\n",
      "‚úÖ Message sent to layer-0, partition 3\n",
      "‚úÖ Message sent to layer-0, partition 4\n",
      "‚úÖ Message sent to layer-0, partition 5\n",
      "‚úÖ Message sent to layer-0, partition 6\n",
      "‚úÖ Message sent to layer-0, partition 7\n",
      "‚úÖ Message sent to layer-0, partition 8\n",
      "‚úÖ Message sent to layer-0, partition 9\n",
      "‚úÖ Message sent to layer-0, partition 0\n",
      "‚úÖ Message sent to layer-0, partition 1\n",
      "‚úÖ Message sent to layer-0, partition 2\n",
      "‚úÖ Message sent to layer-0, partition 3\n",
      "‚úÖ Message sent to layer-0, partition 4\n",
      "‚úÖ Message sent to layer-0, partition 5\n",
      "‚úÖ Message sent to layer-0, partition 6\n",
      "‚úÖ Message sent to layer-0, partition 7\n",
      "‚úÖ Message sent to layer-0, partition 8\n",
      "‚úÖ Message sent to layer-0, partition 9\n",
      "‚úÖ Message sent to layer-0, partition 0\n",
      "‚úÖ Message sent to layer-0, partition 1\n",
      "‚úÖ Message sent to layer-0, partition 2\n",
      "‚úÖ Message sent to layer-0, partition 3\n",
      "‚úÖ Message sent to layer-0, partition 4\n",
      "‚úÖ Message sent to layer-0, partition 5\n",
      "‚úÖ Message sent to layer-0, partition 6\n",
      "‚úÖ Message sent to layer-0, partition 7\n",
      "‚úÖ Message sent to layer-0, partition 8\n",
      "‚úÖ Message sent to layer-0, partition 9\n",
      "‚úÖ Message sent to layer-0, partition 0\n",
      "‚úÖ Message sent to layer-0, partition 1\n",
      "‚úÖ Message sent to layer-0, partition 2\n",
      "‚úÖ Message sent to layer-0, partition 3\n",
      "‚úÖ Message sent to layer-0, partition 4\n",
      "‚úÖ Message sent to layer-0, partition 5\n",
      "‚úÖ Message sent to layer-0, partition 6\n",
      "‚úÖ Message sent to layer-0, partition 7\n",
      "‚úÖ Message sent to layer-0, partition 8\n",
      "‚úÖ Message sent to layer-0, partition 9\n",
      "‚úÖ Message sent to layer-0, partition 0\n",
      "‚úÖ Message sent to layer-0, partition 1\n",
      "‚úÖ Message sent to layer-0, partition 2\n",
      "‚úÖ Message sent to layer-0, partition 3\n",
      "‚úÖ Message sent to layer-0, partition 4\n",
      "‚úÖ Message sent to layer-0, partition 5\n",
      "‚úÖ Message sent to layer-0, partition 6\n",
      "‚úÖ Message sent to layer-0, partition 7\n",
      "‚úÖ Message sent to layer-0, partition 8\n",
      "‚úÖ Message sent to layer-0, partition 9\n",
      "‚úÖ Message sent to layer-0, partition 0\n",
      "‚úÖ Message sent to layer-0, partition 1\n",
      "‚úÖ Message sent to layer-0, partition 2\n",
      "‚úÖ Message sent to layer-0, partition 3\n",
      "‚úÖ Message sent to layer-0, partition 4\n",
      "‚úÖ Message sent to layer-0, partition 5\n",
      "‚úÖ Message sent to layer-0, partition 6\n",
      "‚úÖ Message sent to layer-0, partition 7\n",
      "‚úÖ Message sent to layer-0, partition 8\n",
      "‚úÖ Message sent to layer-0, partition 9\n",
      "‚úÖ Message sent to layer-0, partition 0\n",
      "‚úÖ Message sent to layer-0, partition 1\n",
      "‚úÖ Message sent to layer-0, partition 2\n",
      "‚úÖ Message sent to layer-0, partition 3\n",
      "‚úÖ Message sent to layer-0, partition 4\n",
      "‚úÖ Message sent to layer-0, partition 5\n",
      "‚úÖ Message sent to layer-0, partition 6\n",
      "‚úÖ Message sent to layer-0, partition 7\n",
      "‚úÖ Message sent to layer-0, partition 8\n",
      "‚úÖ Message sent to layer-0, partition 9\n",
      "‚úÖ Message sent to layer-0, partition 0\n",
      "‚úÖ Message sent to layer-0, partition 1\n",
      "‚úÖ Message sent to layer-0, partition 2\n",
      "‚úÖ Message sent to layer-0, partition 3\n",
      "‚úÖ Message sent to layer-0, partition 4\n",
      "‚úÖ Message sent to layer-0, partition 5\n",
      "‚úÖ Message sent to layer-0, partition 6\n",
      "‚úÖ Message sent to layer-0, partition 7\n",
      "Neuron 0 in layer_0 waiting for activation...\n",
      "Neuron 0 received message: {'layer': 'layer_0'}\n",
      "Neuron 0 in layer_0 activated!\n",
      "Neuron 1 in layer_0 waiting for activation...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 184\u001b[0m\n\u001b[1;32m    182\u001b[0m     image, label \u001b[38;5;241m=\u001b[39m mnist_test[i]\n\u001b[1;32m    183\u001b[0m     image_np \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m--> 184\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mforward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Prediction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprediction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Label: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# Calculate accuracy\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 163\u001b[0m, in \u001b[0;36mforward_pass\u001b[0;34m(layers, input_data, image_id)\u001b[0m\n\u001b[1;32m    160\u001b[0m producer\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m layers:\n\u001b[0;32m--> 163\u001b[0m     input_data \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39margmax(input_data))\n\u001b[1;32m    166\u001b[0m redis_client\u001b[38;5;241m.\u001b[39mhset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m'\u001b[39m, image_id, prediction)\n",
      "Cell \u001b[0;32mIn[15], line 111\u001b[0m, in \u001b[0;36mLayer.forward\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    108\u001b[0m producer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    109\u001b[0m producer\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 111\u001b[0m outputs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([neuron\u001b[38;5;241m.\u001b[39mforward(input_data) \u001b[38;5;28;01mfor\u001b[39;00m neuron \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneurons])\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_final_layer:\n\u001b[1;32m    113\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m softmax(outputs)\n",
      "Cell \u001b[0;32mIn[15], line 111\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    108\u001b[0m producer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    109\u001b[0m producer\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 111\u001b[0m outputs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mneuron\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m neuron \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneurons])\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_final_layer:\n\u001b[1;32m    113\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m softmax(outputs)\n",
      "Cell \u001b[0;32mIn[15], line 65\u001b[0m, in \u001b[0;36mNeuron.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     message_received \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m consumer:\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNeuron \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneuron_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m received message: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m message\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mand\u001b[39;00m message\u001b[38;5;241m.\u001b[39mvalue[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_id:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/kafka/consumer/group.py:1197\u001b[0m, in \u001b[0;36mKafkaConsumer.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_v1()\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/kafka/consumer/group.py:1205\u001b[0m, in \u001b[0;36mKafkaConsumer.next_v2\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_message_generator_v2()\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   1207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/kafka/consumer/group.py:1120\u001b[0m, in \u001b[0;36mKafkaConsumer._message_generator_v2\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_message_generator_v2\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1119\u001b[0m     timeout_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consumer_timeout \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mtime())\n\u001b[0;32m-> 1120\u001b[0m     record_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout_ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_offsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tp, records \u001b[38;5;129;01min\u001b[39;00m six\u001b[38;5;241m.\u001b[39miteritems(record_map):\n\u001b[1;32m   1122\u001b[0m         \u001b[38;5;66;03m# Generators are stateful, and it is possible that the tp / records\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m         \u001b[38;5;66;03m# here may become stale during iteration -- i.e., we seek to a\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m         \u001b[38;5;66;03m# different offset, pause consumption, or lose assignment.\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m records:\n\u001b[1;32m   1126\u001b[0m             \u001b[38;5;66;03m# is_fetchable(tp) should handle assignment changes and offset\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m             \u001b[38;5;66;03m# resets; for all other changes (e.g., seeks) we'll rely on the\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m             \u001b[38;5;66;03m# outer function destroying the existing iterator/generator\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m             \u001b[38;5;66;03m# via self._iterator = None\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/kafka/consumer/group.py:657\u001b[0m, in \u001b[0;36mKafkaConsumer.poll\u001b[0;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[1;32m    655\u001b[0m remaining \u001b[38;5;241m=\u001b[39m timeout_ms\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_closed:\n\u001b[0;32m--> 657\u001b[0m     records \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_records\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_offsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupdate_offsets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m records:\n\u001b[1;32m    659\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m records\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/kafka/consumer/group.py:684\u001b[0m, in \u001b[0;36mKafkaConsumer._poll_once\u001b[0;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;66;03m# Fetch positions if we have partitions we're subscribed to that we\u001b[39;00m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;66;03m# don't know the offset for\u001b[39;00m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_subscription\u001b[38;5;241m.\u001b[39mhas_all_fetch_positions():\n\u001b[0;32m--> 684\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fetch_positions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_subscription\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmissing_fetch_positions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;66;03m# If data is available already, e.g. from a previous network client\u001b[39;00m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;66;03m# poll() call to commit, then just return it immediately\u001b[39;00m\n\u001b[1;32m    688\u001b[0m records, partial \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetcher\u001b[38;5;241m.\u001b[39mfetched_records(max_records, update_offsets\u001b[38;5;241m=\u001b[39mupdate_offsets)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/kafka/consumer/group.py:1113\u001b[0m, in \u001b[0;36mKafkaConsumer._update_fetch_positions\u001b[0;34m(self, partitions)\u001b[0m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_subscription\u001b[38;5;241m.\u001b[39mhas_all_fetch_positions():\n\u001b[1;32m   1108\u001b[0m     \u001b[38;5;66;03m# if we still don't have offsets for all partitions, then we should either seek\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m     \u001b[38;5;66;03m# to the last committed position or reset using the auto reset policy\u001b[39;00m\n\u001b[1;32m   1110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapi_version\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m   1111\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroup_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1112\u001b[0m         \u001b[38;5;66;03m# first refresh commits for all assigned partitions\u001b[39;00m\n\u001b[0;32m-> 1113\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_coordinator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrefresh_committed_offsets_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1115\u001b[0m     \u001b[38;5;66;03m# Then, do any offset lookups in case some positions are not known\u001b[39;00m\n\u001b[1;32m   1116\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetcher\u001b[38;5;241m.\u001b[39mupdate_fetch_positions(partitions)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/kafka/coordinator/consumer.py:389\u001b[0m, in \u001b[0;36mConsumerCoordinator.refresh_committed_offsets_if_needed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fetch committed offsets for assigned partitions.\"\"\"\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_subscription\u001b[38;5;241m.\u001b[39mneeds_fetch_committed_offsets:\n\u001b[0;32m--> 389\u001b[0m     offsets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_committed_offsets\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_subscription\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massigned_partitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m partition, offset \u001b[38;5;129;01min\u001b[39;00m six\u001b[38;5;241m.\u001b[39miteritems(offsets):\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;66;03m# verify assignment is still active\u001b[39;00m\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_subscription\u001b[38;5;241m.\u001b[39mis_assigned(partition):\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/kafka/coordinator/consumer.py:421\u001b[0m, in \u001b[0;36mConsumerCoordinator.fetch_committed_offsets\u001b[0;34m(self, partitions)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m future\u001b[38;5;241m.\u001b[39mretriable():\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m future\u001b[38;5;241m.\u001b[39mexception \u001b[38;5;66;03m# pylint: disable-msg=raising-bad-type\u001b[39;00m\n\u001b[0;32m--> 421\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mretry_backoff_ms\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from kafka import KafkaProducer, KafkaConsumer, TopicPartition\n",
    "import redis\n",
    "import time\n",
    "\n",
    "# Kafka settings\n",
    "KAFKA_BROKER = 'kafka:9092'\n",
    "REDIS_HOST = 'host.docker.internal'\n",
    "REDIS_PORT = 6379\n",
    "\n",
    "# Offset setting (latest means only new messages will be consumed)\n",
    "OFFSET_RESET = \"latest\"  # Change to \"earliest\" if debugging\n",
    "\n",
    "# Redis setup\n",
    "redis_client = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, db=0)\n",
    "\n",
    "# Activation functions\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))  # Stability trick\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "ACTIVATIONS = {\n",
    "    \"relu\": relu,\n",
    "    \"softmax\": softmax\n",
    "}\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, layer_id, neuron_id, weights, bias, activation, is_final_layer=False):\n",
    "        self.layer_id = layer_id\n",
    "        self.neuron_id = neuron_id\n",
    "        self.weights = np.array(weights)\n",
    "        self.bias = np.array(bias)\n",
    "        self.activation_func = None if is_final_layer else ACTIVATIONS.get(activation, relu)\n",
    "        self.is_final_layer = is_final_layer\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Wait for activation message from Kafka before processing\"\"\"\n",
    "        topic = f'layer-{self.layer_id[-1]}'\n",
    "        consumer = KafkaConsumer(\n",
    "            bootstrap_servers=KAFKA_BROKER,\n",
    "            value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
    "            auto_offset_reset=OFFSET_RESET,\n",
    "            enable_auto_commit=True,\n",
    "            group_id=f'group-layer-{self.layer_id}',\n",
    "            consumer_timeout_ms=60000  # Increased timeout to avoid early exit\n",
    "        )\n",
    "\n",
    "        # Instead of assigning one partition, listen to ALL partitions of the layer\n",
    "        partitions = consumer.partitions_for_topic(topic)\n",
    "        if partitions:\n",
    "            consumer.assign([TopicPartition(topic, p) for p in partitions])\n",
    "        else:\n",
    "            print(f\"‚ùå ERROR: No partitions found for {topic}\")\n",
    "            return None  # Prevent infinite waiting\n",
    "\n",
    "        print(f\"Neuron {self.neuron_id} in {self.layer_id} waiting for activation...\")\n",
    "\n",
    "        while True:\n",
    "            message_received = False\n",
    "            for message in consumer:\n",
    "                print(f\"Neuron {self.neuron_id} received message: {message.value}\")\n",
    "                if 'layer' in message.value and message.value['layer'] == self.layer_id:\n",
    "                    print(f\"Neuron {self.neuron_id} in {self.layer_id} activated!\")\n",
    "                    message_received = True\n",
    "                    break\n",
    "\n",
    "            if message_received:\n",
    "                consumer.close()\n",
    "                z = np.dot(inputs, self.weights) + self.bias\n",
    "                return z if self.is_final_layer else self.activation_func(z)\n",
    "            \n",
    "            print(f\"‚ö†Ô∏è Neuron {self.neuron_id} in {self.layer_id} still waiting for activation...\")\n",
    "            time.sleep(2)  # Prevents infinite loop from consuming CPU\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, layer_id, neurons, is_final_layer=False):\n",
    "        self.layer_id = layer_id\n",
    "        self.neurons = neurons\n",
    "        self.is_final_layer = is_final_layer\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        \"\"\"Trigger neuron activations via Kafka and activate the next layer when finished\"\"\"\n",
    "        producer = KafkaProducer(\n",
    "            bootstrap_servers=KAFKA_BROKER,\n",
    "            value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    "        )\n",
    "\n",
    "        topic = f'layer-{self.layer_id[-1]}'\n",
    "        print(f\"üì§ Layer {self.layer_id} sending activation messages to topic {topic}...\")\n",
    "\n",
    "        for neuron_id in range(len(self.neurons)):\n",
    "            retries = 5\n",
    "            while retries > 0:\n",
    "                try:\n",
    "                    producer.send(topic, key=str(neuron_id).encode(), value={'layer': self.layer_id}, partition=(neuron_id % 10))\n",
    "                    print(f\"‚úÖ Message sent to {topic}, partition {neuron_id % 10}\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error sending message to neuron {neuron_id}: {e}\")\n",
    "                    retries -= 1\n",
    "                    time.sleep(1)\n",
    "\n",
    "        producer.flush()\n",
    "        producer.close()\n",
    "\n",
    "        outputs = np.array([neuron.forward(input_data) for neuron in self.neurons])\n",
    "        if self.is_final_layer:\n",
    "            outputs = softmax(outputs)\n",
    "\n",
    "        redis_client.set(self.layer_id, outputs.astype(np.float32).tobytes())\n",
    "\n",
    "        # Activate the next layer\n",
    "        producer = KafkaProducer(bootstrap_servers=KAFKA_BROKER, value_serializer=lambda v: json.dumps(v).encode('utf-8'))\n",
    "        next_layer = {'layer': f'layer-{int(self.layer_id[-1]) + 1}'} if not self.is_final_layer else {'layer': 'final'}\n",
    "        producer.send('activate-layer', next_layer)\n",
    "        producer.flush()\n",
    "        producer.close()\n",
    "\n",
    "        return outputs\n",
    "\n",
    "# Load JSON file\n",
    "def load_network(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Build network\n",
    "def build_network(json_data):\n",
    "    layers = []\n",
    "    sorted_layers = sorted(json_data.keys(), key=lambda x: int(x.split('_')[-1]))\n",
    "    for i, layer_name in enumerate(sorted_layers):\n",
    "        layer_info = json_data[layer_name]\n",
    "        neurons = [\n",
    "            Neuron(\n",
    "                layer_id=layer_name,\n",
    "                neuron_id=idx,\n",
    "                weights=np.array(node['weights']),\n",
    "                bias=np.array(node['biases']),\n",
    "                activation=node['activation'],\n",
    "                is_final_layer=(i == len(sorted_layers) - 1)\n",
    "            )\n",
    "            for idx, node in enumerate(layer_info['nodes'])\n",
    "        ]\n",
    "        layers.append(Layer(layer_id=layer_name, neurons=neurons, is_final_layer=(i == len(sorted_layers) - 1)))\n",
    "    return layers\n",
    "\n",
    "# Forward pass for single image\n",
    "def forward_pass(layers, input_data, image_id):\n",
    "    producer = KafkaProducer(\n",
    "        bootstrap_servers=KAFKA_BROKER,\n",
    "        value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    "    )\n",
    "    print(\"üî• Sending initial activation message to layer-0...\")\n",
    "    producer.send('layer-0', {'layer': 'layer-0'})\n",
    "    producer.flush()\n",
    "    producer.close()\n",
    "\n",
    "    for layer in layers:\n",
    "        input_data = layer.forward(input_data)\n",
    "\n",
    "    prediction = int(np.argmax(input_data))\n",
    "    redis_client.hset('predictions', image_id, prediction)\n",
    "    return prediction\n",
    "\n",
    "# Load network\n",
    "data = load_network(\"node_based_model.json\")\n",
    "network = build_network(data)\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "mnist_test = datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "# Test first 10 images one by one\n",
    "for i in range(10):\n",
    "    image, label = mnist_test[i]\n",
    "    image_np = image.view(-1).numpy()\n",
    "    prediction = forward_pass(network, image_np, i)\n",
    "    print(f\"Image {i} Prediction: {prediction}, Label: {label}\")\n",
    "\n",
    "# Calculate accuracy\n",
    "predictions = redis_client.hgetall('predictions')\n",
    "correct = sum(int(predictions[k]) == mnist_test[int(k)][1] for k in predictions)\n",
    "accuracy = correct / len(predictions)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2a6ede-6432-46cb-b7c0-84898cb3e911",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
