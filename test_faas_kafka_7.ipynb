{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a20f3e9-d2d8-4aca-93fa-0eee94b27d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¥ Sending initial activation message to layer-0...\n",
      "ðŸ“¤ Layer layer_0 sending activation messages to topic layer-0...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 158\u001b[0m\n\u001b[1;32m    156\u001b[0m     image, label \u001b[38;5;241m=\u001b[39m mnist_test[i]\n\u001b[1;32m    157\u001b[0m     image_np \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m--> 158\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mforward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Prediction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprediction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Label: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# Calculate accuracy\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 137\u001b[0m, in \u001b[0;36mforward_pass\u001b[0;34m(layers, input_data, image_id)\u001b[0m\n\u001b[1;32m    134\u001b[0m producer\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m layers:\n\u001b[0;32m--> 137\u001b[0m     input_data \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39margmax(input_data))\n\u001b[1;32m    140\u001b[0m redis_client\u001b[38;5;241m.\u001b[39mhset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m'\u001b[39m, image_id, prediction)  \u001b[38;5;66;03m# Store all predictions\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 87\u001b[0m, in \u001b[0;36mLayer.forward\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m     84\u001b[0m producer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m     85\u001b[0m producer\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m---> 87\u001b[0m outputs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([neuron\u001b[38;5;241m.\u001b[39mforward(input_data) \u001b[38;5;28;01mfor\u001b[39;00m neuron \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneurons])\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_final_layer:\n\u001b[1;32m     89\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m softmax(outputs)\n",
      "Cell \u001b[0;32mIn[13], line 87\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     84\u001b[0m producer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m     85\u001b[0m producer\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m---> 87\u001b[0m outputs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mneuron\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m neuron \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneurons])\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_final_layer:\n\u001b[1;32m     89\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m softmax(outputs)\n",
      "Cell \u001b[0;32mIn[13], line 52\u001b[0m, in \u001b[0;36mNeuron.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     49\u001b[0m partition \u001b[38;5;241m=\u001b[39m TopicPartition(topic, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneuron_id)\n\u001b[1;32m     50\u001b[0m consumer\u001b[38;5;241m.\u001b[39massign([partition])\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m consumer:\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Neuron \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneuron_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m received message: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m message\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mand\u001b[39;00m message\u001b[38;5;241m.\u001b[39mvalue[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_id:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/kafka/consumer/group.py:1197\u001b[0m, in \u001b[0;36mKafkaConsumer.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_v1()\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/kafka/consumer/group.py:1205\u001b[0m, in \u001b[0;36mKafkaConsumer.next_v2\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_message_generator_v2()\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   1207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/kafka/consumer/group.py:1120\u001b[0m, in \u001b[0;36mKafkaConsumer._message_generator_v2\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_message_generator_v2\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1119\u001b[0m     timeout_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consumer_timeout \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mtime())\n\u001b[0;32m-> 1120\u001b[0m     record_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout_ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_offsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tp, records \u001b[38;5;129;01min\u001b[39;00m six\u001b[38;5;241m.\u001b[39miteritems(record_map):\n\u001b[1;32m   1122\u001b[0m         \u001b[38;5;66;03m# Generators are stateful, and it is possible that the tp / records\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m         \u001b[38;5;66;03m# here may become stale during iteration -- i.e., we seek to a\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m         \u001b[38;5;66;03m# different offset, pause consumption, or lose assignment.\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m records:\n\u001b[1;32m   1126\u001b[0m             \u001b[38;5;66;03m# is_fetchable(tp) should handle assignment changes and offset\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m             \u001b[38;5;66;03m# resets; for all other changes (e.g., seeks) we'll rely on the\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m             \u001b[38;5;66;03m# outer function destroying the existing iterator/generator\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m             \u001b[38;5;66;03m# via self._iterator = None\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/kafka/consumer/group.py:657\u001b[0m, in \u001b[0;36mKafkaConsumer.poll\u001b[0;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[1;32m    655\u001b[0m remaining \u001b[38;5;241m=\u001b[39m timeout_ms\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_closed:\n\u001b[0;32m--> 657\u001b[0m     records \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_records\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_offsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupdate_offsets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m records:\n\u001b[1;32m    659\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m records\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/kafka/consumer/group.py:706\u001b[0m, in \u001b[0;36mKafkaConsumer._poll_once\u001b[0;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mpoll(timeout_ms\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    705\u001b[0m timeout_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(timeout_ms, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coordinator\u001b[38;5;241m.\u001b[39mtime_to_next_poll() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m--> 706\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout_ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;66;03m# after the long poll, we should check whether the group needs to rebalance\u001b[39;00m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;66;03m# prior to returning data so that the group can stabilize faster\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coordinator\u001b[38;5;241m.\u001b[39mneed_rejoin():\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/kafka/client_async.py:614\u001b[0m, in \u001b[0;36mKafkaClient.poll\u001b[0;34m(self, timeout_ms, future)\u001b[0m\n\u001b[1;32m    607\u001b[0m         timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\n\u001b[1;32m    608\u001b[0m             timeout_ms,\n\u001b[1;32m    609\u001b[0m             metadata_timeout_ms,\n\u001b[1;32m    610\u001b[0m             idle_connection_timeout_ms,\n\u001b[1;32m    611\u001b[0m             request_timeout_ms)\n\u001b[1;32m    612\u001b[0m         timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, timeout)  \u001b[38;5;66;03m# avoid negative timeouts\u001b[39;00m\n\u001b[0;32m--> 614\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;66;03m# called without the lock to avoid deadlock potential\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;66;03m# if handlers need to acquire locks\u001b[39;00m\n\u001b[1;32m    618\u001b[0m responses\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fire_pending_completed_requests())\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/kafka/client_async.py:648\u001b[0m, in \u001b[0;36mKafkaClient._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_register_send_sockets()\n\u001b[1;32m    647\u001b[0m start_select \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 648\u001b[0m ready \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    649\u001b[0m end_select \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sensors:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/selectors.py:469\u001b[0m, in \u001b[0;36mEpollSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    467\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 469\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ev\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from kafka import KafkaProducer, KafkaConsumer, TopicPartition\n",
    "import redis\n",
    "import time\n",
    "\n",
    "offset = \"latest\"\n",
    "\n",
    "# Redis setup\n",
    "redis_client = redis.Redis(host='host.docker.internal', port=6379, db=0)\n",
    "\n",
    "# Activation functions\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))  # Stability trick\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "ACTIVATIONS = {\n",
    "    \"relu\": relu,\n",
    "    \"softmax\": softmax\n",
    "}\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, layer_id, neuron_id, weights, bias, activation, is_final_layer=False):\n",
    "        self.layer_id = layer_id\n",
    "        self.neuron_id = neuron_id\n",
    "        self.weights = np.array(weights)\n",
    "        self.bias = np.array(bias)\n",
    "        self.activation_func = None if is_final_layer else ACTIVATIONS.get(activation, relu)\n",
    "        self.is_final_layer = is_final_layer\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Wait for activation message from Kafka before processing\"\"\"\n",
    "        topic = f'layer-{self.layer_id[-1]}'\n",
    "        consumer = KafkaConsumer(\n",
    "            bootstrap_servers='kafka:9092',\n",
    "            value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
    "            auto_offset_reset=offset,  # âœ… Always read new messages\n",
    "            enable_auto_commit=True,\n",
    "            group_id=f'group-layer-{self.layer_id}',\n",
    "            consumer_timeout_ms=120000\n",
    "        )\n",
    "\n",
    "        partition = TopicPartition(topic, self.neuron_id)\n",
    "        consumer.assign([partition])\n",
    "\n",
    "        for message in consumer:\n",
    "            print(f\"âœ… Neuron {self.neuron_id} received message: {message.value}\")\n",
    "            if 'layer' in message.value and message.value['layer'] == self.layer_id:\n",
    "                print(f\"ðŸš€ Neuron {self.neuron_id} in {self.layer_id} activated!\")\n",
    "                break\n",
    "\n",
    "        consumer.close()\n",
    "\n",
    "        z = np.dot(inputs, self.weights) + self.bias\n",
    "        return z if self.is_final_layer else self.activation_func(z)\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, layer_id, neurons, is_final_layer=False):\n",
    "        self.layer_id = layer_id\n",
    "        self.neurons = neurons\n",
    "        self.is_final_layer = is_final_layer\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        \"\"\"Trigger neuron activations via Kafka and activate the next layer when finished\"\"\"\n",
    "        producer = KafkaProducer(bootstrap_servers='kafka:9092',\n",
    "                                 value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
    "                                 retries=5, request_timeout_ms=10000)\n",
    "    \n",
    "        topic = f'layer-{self.layer_id[-1]}'\n",
    "        activation_message = {'layer': self.layer_id}\n",
    "\n",
    "        print(f\"ðŸ“¤ Layer {self.layer_id} sending activation messages to topic {topic}...\")\n",
    "\n",
    "        for neuron_id in range(len(self.neurons)):\n",
    "            producer.send(topic, key=str(neuron_id).encode(), value=activation_message, partition=neuron_id)\n",
    "            #print(f\"âœ… Message sent to {topic}, partition {neuron_id}\")\n",
    "\n",
    "        producer.flush()\n",
    "        producer.close()\n",
    "\n",
    "        outputs = np.array([neuron.forward(input_data) for neuron in self.neurons])\n",
    "        if self.is_final_layer:\n",
    "            outputs = softmax(outputs)\n",
    "        \n",
    "        redis_client.set(self.layer_id, outputs.astype(np.float32).tobytes())\n",
    "        \n",
    "        # Activate the next layer\n",
    "        producer = KafkaProducer(bootstrap_servers='kafka:9092',\n",
    "                                 value_serializer=lambda v: json.dumps(v).encode('utf-8'))\n",
    "        producer.send('activate-layer', {'layer': f'layer-{int(self.layer_id[-1]) + 1}'} if not self.is_final_layer else {'layer': 'final'})\n",
    "        producer.flush()\n",
    "        producer.close()\n",
    "    \n",
    "        return outputs\n",
    "\n",
    "# Load JSON file\n",
    "def load_network(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Build network\n",
    "def build_network(json_data):\n",
    "    layers = []\n",
    "    sorted_layers = sorted(json_data.keys(), key=lambda x: int(x.split('_')[-1]))\n",
    "    for i, layer_name in enumerate(sorted_layers):\n",
    "        layer_info = json_data[layer_name]\n",
    "        neurons = [\n",
    "            Neuron(\n",
    "                layer_id=layer_name,\n",
    "                neuron_id=idx,\n",
    "                weights=np.array(node['weights']),\n",
    "                bias=np.array(node['biases']),\n",
    "                activation=node['activation'],\n",
    "                is_final_layer=(i == len(sorted_layers) - 1)\n",
    "            )\n",
    "            for idx, node in enumerate(layer_info['nodes'])\n",
    "        ]\n",
    "        layers.append(Layer(layer_id=layer_name, neurons=neurons, is_final_layer=(i == len(sorted_layers) - 1)))\n",
    "    return layers\n",
    "\n",
    "# Forward pass for single image\n",
    "def forward_pass(layers, input_data, image_id):\n",
    "    producer = KafkaProducer(bootstrap_servers='kafka:9092',\n",
    "                             value_serializer=lambda v: json.dumps(v).encode('utf-8'))\n",
    "    print(\"ðŸ”¥ Sending initial activation message to layer-0...\")\n",
    "    producer.send('layer-0', {'layer': 'layer_0'})\n",
    "    producer.flush()\n",
    "    producer.close()\n",
    "    \n",
    "    for layer in layers:\n",
    "        input_data = layer.forward(input_data)\n",
    "    \n",
    "    prediction = int(np.argmax(input_data))\n",
    "    redis_client.hset('predictions', image_id, prediction)  # Store all predictions\n",
    "    return prediction\n",
    "\n",
    "# Load network\n",
    "data = load_network(\"node_based_model.json\")\n",
    "network = build_network(data)\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "mnist_test = datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "# Test first 10 images one by one\n",
    "for i in range(10):\n",
    "    image, label = mnist_test[i]\n",
    "    image_np = image.view(-1).numpy()\n",
    "    prediction = forward_pass(network, image_np, i)\n",
    "    print(f\"Image {i} Prediction: {prediction}, Label: {label}\")\n",
    "\n",
    "# Calculate accuracy\n",
    "predictions = redis_client.hgetall('predictions')\n",
    "correct = sum(int(predictions[k]) == mnist_test[int(k)][1] for k in predictions)\n",
    "accuracy = correct / len(predictions)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35a8d1c0-2f9c-4967-bbcd-17fb37139e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¥ Sending initial activation message to activate-layer topic...\n",
      "Layer layer_0 waiting for activation message...\n",
      "Neuron 0 in layer-0 activated\n",
      "Neuron 1 in layer-0 activated\n",
      "Neuron 2 in layer-0 activated\n",
      "Neuron 3 in layer-0 activated\n",
      "Neuron 4 in layer-0 activated\n",
      "Neuron 5 in layer-0 activated\n",
      "Neuron 6 in layer-0 activated\n",
      "Neuron 7 in layer-0 activated\n",
      "Neuron 8 in layer-0 activated\n",
      "Neuron 9 in layer-0 activated\n",
      "Neuron 10 in layer-0 activated\n",
      "Neuron 11 in layer-0 activated\n",
      "Neuron 12 in layer-0 activated\n",
      "Neuron 13 in layer-0 activated\n",
      "Neuron 14 in layer-0 activated\n",
      "Neuron 15 in layer-0 activated\n",
      "Neuron 16 in layer-0 activated\n",
      "Neuron 17 in layer-0 activated\n",
      "Neuron 18 in layer-0 activated\n",
      "Neuron 19 in layer-0 activated\n",
      "Neuron 20 in layer-0 activated\n",
      "Neuron 21 in layer-0 activated\n",
      "Neuron 22 in layer-0 activated\n",
      "Neuron 23 in layer-0 activated\n",
      "Neuron 24 in layer-0 activated\n",
      "Neuron 25 in layer-0 activated\n",
      "Neuron 26 in layer-0 activated\n",
      "Neuron 27 in layer-0 activated\n",
      "Neuron 28 in layer-0 activated\n",
      "Neuron 29 in layer-0 activated\n",
      "Neuron 30 in layer-0 activated\n",
      "Neuron 31 in layer-0 activated\n",
      "Neuron 32 in layer-0 activated\n",
      "Neuron 33 in layer-0 activated\n",
      "Neuron 34 in layer-0 activated\n",
      "Neuron 35 in layer-0 activated\n",
      "Neuron 36 in layer-0 activated\n",
      "Neuron 37 in layer-0 activated\n",
      "Neuron 38 in layer-0 activated\n",
      "Neuron 39 in layer-0 activated\n",
      "Neuron 40 in layer-0 activated\n",
      "Neuron 41 in layer-0 activated\n",
      "Neuron 42 in layer-0 activated\n",
      "Neuron 43 in layer-0 activated\n",
      "Neuron 44 in layer-0 activated\n",
      "Neuron 45 in layer-0 activated\n",
      "Neuron 46 in layer-0 activated\n",
      "Neuron 47 in layer-0 activated\n",
      "Neuron 48 in layer-0 activated\n",
      "Neuron 49 in layer-0 activated\n",
      "Neuron 50 in layer-0 activated\n",
      "Neuron 51 in layer-0 activated\n",
      "Neuron 52 in layer-0 activated\n",
      "Neuron 53 in layer-0 activated\n",
      "Neuron 54 in layer-0 activated\n",
      "Neuron 55 in layer-0 activated\n",
      "Neuron 56 in layer-0 activated\n",
      "Neuron 57 in layer-0 activated\n",
      "Neuron 58 in layer-0 activated\n",
      "Neuron 59 in layer-0 activated\n",
      "Neuron 60 in layer-0 activated\n",
      "Neuron 61 in layer-0 activated\n",
      "Neuron 62 in layer-0 activated\n",
      "Neuron 63 in layer-0 activated\n",
      "Neuron 64 in layer-0 activated\n",
      "Neuron 65 in layer-0 activated\n",
      "Neuron 66 in layer-0 activated\n",
      "Neuron 67 in layer-0 activated\n",
      "Neuron 68 in layer-0 activated\n",
      "Neuron 69 in layer-0 activated\n",
      "Neuron 70 in layer-0 activated\n",
      "Neuron 71 in layer-0 activated\n",
      "Neuron 72 in layer-0 activated\n",
      "Neuron 73 in layer-0 activated\n",
      "Neuron 74 in layer-0 activated\n",
      "Neuron 75 in layer-0 activated\n",
      "Neuron 76 in layer-0 activated\n",
      "Neuron 77 in layer-0 activated\n",
      "Neuron 78 in layer-0 activated\n",
      "Neuron 79 in layer-0 activated\n",
      "Neuron 80 in layer-0 activated\n",
      "Neuron 81 in layer-0 activated\n",
      "Neuron 82 in layer-0 activated\n",
      "Neuron 83 in layer-0 activated\n",
      "Neuron 84 in layer-0 activated\n",
      "Neuron 85 in layer-0 activated\n",
      "Neuron 86 in layer-0 activated\n",
      "Neuron 87 in layer-0 activated\n",
      "Neuron 88 in layer-0 activated\n",
      "Neuron 89 in layer-0 activated\n",
      "Neuron 90 in layer-0 activated\n",
      "Neuron 91 in layer-0 activated\n",
      "Neuron 92 in layer-0 activated\n",
      "Neuron 93 in layer-0 activated\n",
      "Neuron 94 in layer-0 activated\n",
      "Neuron 95 in layer-0 activated\n",
      "Neuron 96 in layer-0 activated\n",
      "Neuron 97 in layer-0 activated\n",
      "Neuron 98 in layer-0 activated\n",
      "Neuron 99 in layer-0 activated\n",
      "Neuron 100 in layer-0 activated\n",
      "Neuron 101 in layer-0 activated\n",
      "Neuron 102 in layer-0 activated\n",
      "Neuron 103 in layer-0 activated\n",
      "Neuron 104 in layer-0 activated\n",
      "Neuron 105 in layer-0 activated\n",
      "Neuron 106 in layer-0 activated\n",
      "Neuron 107 in layer-0 activated\n",
      "Neuron 108 in layer-0 activated\n",
      "Neuron 109 in layer-0 activated\n",
      "Neuron 110 in layer-0 activated\n",
      "Neuron 111 in layer-0 activated\n",
      "Neuron 112 in layer-0 activated\n",
      "Neuron 113 in layer-0 activated\n",
      "Neuron 114 in layer-0 activated\n",
      "Neuron 115 in layer-0 activated\n",
      "Neuron 116 in layer-0 activated\n",
      "Neuron 117 in layer-0 activated\n",
      "Neuron 118 in layer-0 activated\n",
      "Neuron 119 in layer-0 activated\n",
      "Neuron 120 in layer-0 activated\n",
      "Neuron 121 in layer-0 activated\n",
      "Neuron 122 in layer-0 activated\n",
      "Neuron 123 in layer-0 activated\n",
      "Neuron 124 in layer-0 activated\n",
      "Neuron 125 in layer-0 activated\n",
      "Neuron 126 in layer-0 activated\n",
      "Neuron 127 in layer-0 activated\n",
      "Neuron 0 in layer_0 waiting for activation...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 182\u001b[0m\n\u001b[1;32m    180\u001b[0m     image, label \u001b[38;5;241m=\u001b[39m mnist_test[i]\n\u001b[1;32m    181\u001b[0m     image_np \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m--> 182\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mforward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Prediction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprediction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Label: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# Calculate accuracy\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 135\u001b[0m, in \u001b[0;36mforward_pass\u001b[0;34m(layers, input_data, image_id)\u001b[0m\n\u001b[1;32m    132\u001b[0m producer\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m layers:\n\u001b[0;32m--> 135\u001b[0m     input_data \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39margmax(input_data))\n\u001b[1;32m    138\u001b[0m redis_handler\u001b[38;5;241m.\u001b[39mhset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m'\u001b[39m, image_id, prediction)\n",
      "Cell \u001b[0;32mIn[1], line 107\u001b[0m, in \u001b[0;36mLayer.forward\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    104\u001b[0m     producer\u001b[38;5;241m.\u001b[39msend(topic, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(neuron_id)\u001b[38;5;241m.\u001b[39mencode(), value\u001b[38;5;241m=\u001b[39mactivation_message)\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNeuron \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mneuron_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtopic\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m activated\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 107\u001b[0m outputs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([neuron\u001b[38;5;241m.\u001b[39mforward(input_data) \u001b[38;5;28;01mfor\u001b[39;00m neuron \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneurons])\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_final_layer:\n\u001b[1;32m    109\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m softmax(outputs)\n",
      "Cell \u001b[0;32mIn[1], line 107\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    104\u001b[0m     producer\u001b[38;5;241m.\u001b[39msend(topic, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(neuron_id)\u001b[38;5;241m.\u001b[39mencode(), value\u001b[38;5;241m=\u001b[39mactivation_message)\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNeuron \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mneuron_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtopic\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m activated\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 107\u001b[0m outputs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mneuron\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m neuron \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneurons])\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_final_layer:\n\u001b[1;32m    109\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m softmax(outputs)\n",
      "Cell \u001b[0;32mIn[1], line 54\u001b[0m, in \u001b[0;36mNeuron.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     50\u001b[0m consumer\u001b[38;5;241m.\u001b[39massign([partition])\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNeuron \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneuron_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m waiting for activation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m consumer:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m message\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_id:\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNeuron \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneuron_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m activated with message: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/kafka/consumer/group.py:1197\u001b[0m, in \u001b[0;36mKafkaConsumer.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_v1()\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/kafka/consumer/group.py:1205\u001b[0m, in \u001b[0;36mKafkaConsumer.next_v2\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_message_generator_v2()\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   1207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/kafka/consumer/group.py:1120\u001b[0m, in \u001b[0;36mKafkaConsumer._message_generator_v2\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_message_generator_v2\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1119\u001b[0m     timeout_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consumer_timeout \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mtime())\n\u001b[0;32m-> 1120\u001b[0m     record_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout_ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_offsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tp, records \u001b[38;5;129;01min\u001b[39;00m six\u001b[38;5;241m.\u001b[39miteritems(record_map):\n\u001b[1;32m   1122\u001b[0m         \u001b[38;5;66;03m# Generators are stateful, and it is possible that the tp / records\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m         \u001b[38;5;66;03m# here may become stale during iteration -- i.e., we seek to a\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m         \u001b[38;5;66;03m# different offset, pause consumption, or lose assignment.\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m records:\n\u001b[1;32m   1126\u001b[0m             \u001b[38;5;66;03m# is_fetchable(tp) should handle assignment changes and offset\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m             \u001b[38;5;66;03m# resets; for all other changes (e.g., seeks) we'll rely on the\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m             \u001b[38;5;66;03m# outer function destroying the existing iterator/generator\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m             \u001b[38;5;66;03m# via self._iterator = None\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/kafka/consumer/group.py:657\u001b[0m, in \u001b[0;36mKafkaConsumer.poll\u001b[0;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[1;32m    655\u001b[0m remaining \u001b[38;5;241m=\u001b[39m timeout_ms\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_closed:\n\u001b[0;32m--> 657\u001b[0m     records \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_records\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_offsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupdate_offsets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m records:\n\u001b[1;32m    659\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m records\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/kafka/consumer/group.py:706\u001b[0m, in \u001b[0;36mKafkaConsumer._poll_once\u001b[0;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mpoll(timeout_ms\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    705\u001b[0m timeout_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(timeout_ms, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coordinator\u001b[38;5;241m.\u001b[39mtime_to_next_poll() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m--> 706\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout_ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;66;03m# after the long poll, we should check whether the group needs to rebalance\u001b[39;00m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;66;03m# prior to returning data so that the group can stabilize faster\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coordinator\u001b[38;5;241m.\u001b[39mneed_rejoin():\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/kafka/client_async.py:608\u001b[0m, in \u001b[0;36mKafkaClient.poll\u001b[0;34m(self, timeout_ms, future)\u001b[0m\n\u001b[1;32m    601\u001b[0m         timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\n\u001b[1;32m    602\u001b[0m             timeout_ms,\n\u001b[1;32m    603\u001b[0m             metadata_timeout_ms,\n\u001b[1;32m    604\u001b[0m             idle_connection_timeout_ms,\n\u001b[1;32m    605\u001b[0m             request_timeout_ms)\n\u001b[1;32m    606\u001b[0m         timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, timeout)  \u001b[38;5;66;03m# avoid negative timeouts\u001b[39;00m\n\u001b[0;32m--> 608\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# called without the lock to avoid deadlock potential\u001b[39;00m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;66;03m# if handlers need to acquire locks\u001b[39;00m\n\u001b[1;32m    612\u001b[0m responses\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fire_pending_completed_requests())\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/kafka/client_async.py:647\u001b[0m, in \u001b[0;36mKafkaClient._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_register_send_sockets()\n\u001b[1;32m    646\u001b[0m start_select \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 647\u001b[0m ready \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    648\u001b[0m end_select \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sensors:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/selectors.py:469\u001b[0m, in \u001b[0;36mEpollSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    467\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 469\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ev\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from kafka import KafkaProducer, KafkaConsumer, TopicPartition\n",
    "from pcomp.redis_utils import RedisHandler\n",
    "import uuid\n",
    "import time\n",
    "\n",
    "# settings\n",
    "KAFKA_BROKER = 'kafka:9092'\n",
    "REDIS_HOST = 'host.docker.internal'\n",
    "REDIS_PORT = 6379\n",
    "OFFSET_RESET = \"latest\"\n",
    "\n",
    "redis_handler = RedisHandler(REDIS_HOST, REDIS_PORT, 0)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "ACTIVATIONS = {\"relu\": relu, \"softmax\": softmax}\n",
    "\n",
    "def generate_group_id():\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, layer_id, neuron_id, weights, bias, activation, is_final_layer=False):\n",
    "        self.layer_id = layer_id\n",
    "        self.neuron_id = neuron_id\n",
    "        self.weights = np.array(weights)\n",
    "        self.bias = np.array(bias)\n",
    "        self.activation_func = None if is_final_layer else ACTIVATIONS.get(activation, relu)\n",
    "        self.is_final_layer = is_final_layer\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        topic = f'layer-{self.layer_id[-1]}'\n",
    "        consumer = KafkaConsumer(\n",
    "            bootstrap_servers=KAFKA_BROKER,\n",
    "            value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
    "            auto_offset_reset=OFFSET_RESET,\n",
    "            enable_auto_commit=True,\n",
    "            group_id=generate_group_id(),\n",
    "            consumer_timeout_ms=120000\n",
    "        )\n",
    "\n",
    "        partition = TopicPartition(topic, self.neuron_id)\n",
    "        consumer.assign([partition])\n",
    "\n",
    "        print(f\"Neuron {self.neuron_id} in {self.layer_id} waiting for activation...\")\n",
    "\n",
    "        for message in consumer:\n",
    "            if message.value.get('layer') == self.layer_id:\n",
    "                print(f\"Neuron {self.neuron_id} in {self.layer_id} activated with message: {message.value}\")\n",
    "                break\n",
    "\n",
    "        consumer.close()\n",
    "\n",
    "        z = np.dot(inputs, self.weights) + self.bias\n",
    "        return z if self.is_final_layer else self.activation_func(z)\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, layer_id, neurons, is_final_layer=False):\n",
    "        self.layer_id = layer_id\n",
    "        self.neurons = neurons\n",
    "        self.is_final_layer = is_final_layer\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        consumer = KafkaConsumer(\n",
    "            'activate-layer',\n",
    "            bootstrap_servers=KAFKA_BROKER,\n",
    "            value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
    "            auto_offset_reset=OFFSET_RESET,\n",
    "            enable_auto_commit=False,\n",
    "            group_id=generate_group_id(),\n",
    "            consumer_timeout_ms=120000\n",
    "        )\n",
    "\n",
    "        consumer.subscribe(['activate-layer'])\n",
    "        print(f\"Layer {self.layer_id} waiting for activation message...\")\n",
    "\n",
    "        for message in consumer:\n",
    "            if message.value.get('layer') == self.layer_id:\n",
    "                print(f\"Layer {self.layer_id} activated with message: {message.value}\")\n",
    "                consumer.commit()\n",
    "                break\n",
    "\n",
    "        consumer.close()\n",
    "\n",
    "        # Clear old messages in activate-layer topic\n",
    "        producer = KafkaProducer(\n",
    "            bootstrap_servers=KAFKA_BROKER,\n",
    "            value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    "        )\n",
    "        #producer.send('activate-layer', {'layer': None})  # Dummy message to clear old activations\n",
    "\n",
    "        # Activate neurons via Kafka\n",
    "        topic = f'layer-{self.layer_id[-1]}'\n",
    "        activation_message = {'layer': self.layer_id}\n",
    "\n",
    "        for neuron_id in range(len(self.neurons)):\n",
    "            producer.send(topic, key=str(neuron_id).encode(), value=activation_message)\n",
    "            print(f\"Neuron {neuron_id} in {topic} activated\")\n",
    "\n",
    "        outputs = np.array([neuron.forward(input_data) for neuron in self.neurons])\n",
    "        if self.is_final_layer:\n",
    "            outputs = softmax(outputs)\n",
    "\n",
    "        redis_handler.set(self.layer_id, outputs.astype(np.float32).tobytes())\n",
    "\n",
    "        # Activate the next layer\n",
    "        if not self.is_final_layer:\n",
    "            next_layer = f'layer-{int(self.layer_id[-1]) + 1}'\n",
    "            print(f\"Activating next layer: {next_layer}\")\n",
    "            producer.send('activate-layer', {'layer': next_layer})\n",
    "\n",
    "        producer.flush()\n",
    "        producer.close()\n",
    "\n",
    "        return outputs\n",
    "\n",
    "def forward_pass(layers, input_data, image_id):\n",
    "    producer = KafkaProducer(\n",
    "        bootstrap_servers=KAFKA_BROKER,\n",
    "        value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    "    )\n",
    "    print(\"ðŸ”¥ Sending initial activation message to activate-layer topic...\")\n",
    "    producer.send('activate-layer', {'layer': layers[0].layer_id})\n",
    "    producer.flush()\n",
    "    producer.close()\n",
    "\n",
    "    for layer in layers:\n",
    "        input_data = layer.forward(input_data)\n",
    "\n",
    "    prediction = int(np.argmax(input_data))\n",
    "    redis_handler.hset('predictions', image_id, prediction)\n",
    "    return prediction\n",
    "\n",
    "\n",
    "# Load JSON file\n",
    "def load_network(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Build network\n",
    "def build_network(json_data):\n",
    "    layers = []\n",
    "    sorted_layers = sorted(json_data.keys(), key=lambda x: int(x.split('_')[-1]))\n",
    "    for i, layer_name in enumerate(sorted_layers):\n",
    "        layer_info = json_data[layer_name]\n",
    "        neurons = [\n",
    "            Neuron(\n",
    "                layer_id=layer_name,\n",
    "                neuron_id=idx,\n",
    "                weights=np.array(node['weights']),\n",
    "                bias=np.array(node['biases']),\n",
    "                activation=node['activation'],\n",
    "                is_final_layer=(i == len(sorted_layers) - 1)\n",
    "            )\n",
    "            for idx, node in enumerate(layer_info['nodes'])\n",
    "        ]\n",
    "        layers.append(Layer(layer_id=layer_name, neurons=neurons, is_final_layer=(i == len(sorted_layers) - 1)))\n",
    "    return layers\n",
    "        \n",
    "# Load network\n",
    "data = load_network(\"node_based_model.json\")\n",
    "network = build_network(data)\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "mnist_test = datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "# Test first 10 images one by one\n",
    "for i in range(10):\n",
    "    image, label = mnist_test[i]\n",
    "    image_np = image.view(-1).numpy()\n",
    "    prediction = forward_pass(network, image_np, i)\n",
    "    print(f\"Image {i} Prediction: {prediction}, Label: {label}\")\n",
    "\n",
    "# Calculate accuracy\n",
    "predictions = redis_handler.hgetall('predictions')\n",
    "correct = sum(int(predictions[k]) == mnist_test[int(k)][1] for k in predictions)\n",
    "accuracy = correct / len(predictions)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fab385-d8a9-41ed-bfb7-97f1b464b2ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
