{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b4746c8-abd3-473a-b050-2580a2be4e21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import redis \n",
    "\n",
    "client = redis.Redis('host.docker.internal', 6379, 0)\n",
    "\n",
    "client.flushdb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e839f3a3-f827-4d85-b49e-d6fc3df00357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import threading\n",
    "import time\n",
    "import io\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pcomp.kafka_confluent_utils import KafkaProducerHandler, KafkaConsumerHandler\n",
    "from pcomp.activation_functions import ACTIVATIONS, relu, softmax\n",
    "from pcomp.redis_utils import RedisHandler\n",
    "from pcomp.s3client import S3Client\n",
    "from pcomp.utils import batch_generator\n",
    "from pcomp import neuroncalc\n",
    "from base64 import b64encode, b64decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "910fbab6-325f-4103-b1f1-c0a1dc385513",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threads started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7faf6ab32190>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 201\u001b[0m\n\u001b[1;32m    197\u001b[0m     thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThreads started\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 201\u001b[0m \u001b[43mpredict_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 175\u001b[0m, in \u001b[0;36mpredict_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m    173\u001b[0m buffer \u001b[38;5;241m=\u001b[39m s3_client\u001b[38;5;241m.\u001b[39mdownload_fileobj(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy-bucket\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmnist.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    174\u001b[0m content_str \u001b[38;5;241m=\u001b[39m buffer\u001b[38;5;241m.\u001b[39mgetvalue()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 175\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenfromtxt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStringIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent_str\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_header\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m features \u001b[38;5;241m=\u001b[39m data[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][:\u001b[38;5;241m500\u001b[39m]\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_generator(features, batch_size), start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/numpy/lib/_npyio_impl.py:2268\u001b[0m, in \u001b[0;36mgenfromtxt\u001b[0;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding, ndmin, like)\u001b[0m\n\u001b[1;32m   2266\u001b[0m \u001b[38;5;66;03m# Parse each line\u001b[39;00m\n\u001b[1;32m   2267\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (i, line) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(itertools\u001b[38;5;241m.\u001b[39mchain([first_line, ], fhd)):\n\u001b[0;32m-> 2268\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43msplit_line\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2269\u001b[0m     nbvalues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(values)\n\u001b[1;32m   2270\u001b[0m     \u001b[38;5;66;03m# Skip an empty line\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/numpy/lib/_iotools.py:226\u001b[0m, in \u001b[0;36mLineSplitter.__call__\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, line):\n\u001b[0;32m--> 226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handyman\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_decode_line\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/numpy/lib/_iotools.py:205\u001b[0m, in \u001b[0;36mLineSplitter._delimited_splitter\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line:\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelimiter\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Kafka Configuration\n",
    "KAFKA_BROKER = 'kafka:9092'\n",
    "\n",
    "class Neuron(threading.Thread):\n",
    "    def __init__(self, layer_id, neuron_id, weights, bias, activation, is_final_layer):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.layer_id = layer_id\n",
    "        self.layer_id_num = int(self.layer_id.replace(\"layer_\", \"\"))\n",
    "        self.neuron_id = neuron_id\n",
    "        self.weights = np.array(weights)\n",
    "        self.bias = np.array(bias)\n",
    "        self.activation = activation\n",
    "        self.activation_func = ACTIVATIONS.get(activation, relu)\n",
    "        self.is_final_layer = is_final_layer\n",
    "        self.redis_handler = RedisHandler('host.docker.internal', 6379, 0)\n",
    "        self.executor = ThreadPoolExecutor(max_workers=4)\n",
    "        self.producer = None\n",
    "\n",
    "    def fetch_input(self, batch_id, batch_size, columns_size):\n",
    "        key = f\"batch:{batch_id}:initial_data\" if self.layer_id == 'layer_0' else f\"batch:{batch_id}:layer_{int(self.layer_id[-1]) - 1}\"\n",
    "        # Poll Redis until the data is available.\n",
    "        while True:\n",
    "            data = np.frombuffer(self.redis_handler.get(key), dtype=np.float64).reshape(-1, int(columns_size))\n",
    "            if data is not None:\n",
    "                return data\n",
    "            print(f\"‚è≥ Neuron {self.neuron_id} waiting for input data for key: {key}\")\n",
    "\n",
    "    def process_and_send(self, batch_id, batch_size, columns_size):\n",
    "        input_data = self.fetch_input(batch_id, batch_size, columns_size)\n",
    "        # z = np.dot(input_data, self.weights) + self.bias\n",
    "        # output = z if self.is_final_layer else self.activation_func(z)\n",
    "        activation_fn = \"None\" if self.is_final_layer else self.activation\n",
    "        output = neuroncalc.compute_neuron_output_batch(\n",
    "            input_data, self.weights, self.bias,\n",
    "            activation_fn\n",
    "        )\n",
    "        #self.redis_handler.set(f\"batch:{batch_id}:n_{self.layer_id_num}_{self.neuron_id}\", output, True, 1000)\n",
    "        msg = f\"{self.neuron_id}|{batch_id}|{batch_size}|{columns_size}|{b64encode(output.tobytes()).decode('utf-8')}\"\n",
    "        self.producer.send(f'layer-{self.layer_id_num}-complete', msg, self.layer_id_num)\n",
    "        #self.producer.send(f'requests-responses', 'www.neuron.example')\n",
    "\n",
    "    def run(self):\n",
    "        # Instantiate Kafka consumer and producer inside the thread.\n",
    "        consumer = KafkaConsumerHandler(f'layer-{self.layer_id_num}', KAFKA_BROKER, partition=self.neuron_id)\n",
    "        self.producer = KafkaProducerHandler(KAFKA_BROKER)\n",
    "        last_msg_time = time.time()\n",
    "        while True:\n",
    "            got_message = False\n",
    "            for message in consumer.consume():\n",
    "                got_message = True\n",
    "                last_msg_time = time.time()\n",
    "                layer, batch_id_str, batch_size, columns_size = message.split('|')\n",
    "                if layer == self.layer_id:\n",
    "                    batch_id = int(batch_id_str)\n",
    "                    self.executor.submit(self.process_and_send, batch_id, batch_size, columns_size)\n",
    "            if not got_message and (time.time() - last_msg_time > 10):\n",
    "                consumer.commit()\n",
    "                consumer.close()\n",
    "                self.producer.close()\n",
    "                break\n",
    "            time.sleep(0.05)\n",
    "\n",
    "\n",
    "class LayerCoordinator(threading.Thread):\n",
    "    def __init__(self, layer_id, neuron_count, is_final_layer=False):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.layer_id = layer_id\n",
    "        self.layer_id_num = int(self.layer_id.replace(\"layer_\", \"\"))\n",
    "        self.neuron_count = neuron_count\n",
    "        self.is_final_layer = is_final_layer\n",
    "        #self.batch_keys = {}\n",
    "        self.outputs = {}\n",
    "        self.completed_count = {}\n",
    "        self.redis_handler = RedisHandler('host.docker.internal', 6379, 0)\n",
    "        self.executor = ThreadPoolExecutor(max_workers=4)\n",
    "        self.producer = None\n",
    "\n",
    "    def run(self):\n",
    "        consumer = KafkaConsumerHandler(f'layer-{self.layer_id_num}-complete', KAFKA_BROKER, self.layer_id_num)\n",
    "        self.producer = KafkaProducerHandler(KAFKA_BROKER)\n",
    "        last_msg_time = time.time()\n",
    "        while True:\n",
    "            got_message = False\n",
    "            for message in consumer.consume():\n",
    "                got_message = True\n",
    "                last_msg_time = time.time()\n",
    "                neuron_id, batch_id, batch_size, columns_size, arr_base64_str = message.split('|')\n",
    "                neuron_id = int(neuron_id)\n",
    "                batch_id = int(batch_id)\n",
    "                arr_bytes = b64decode(arr_base64_str)\n",
    "                output = np.frombuffer(arr_bytes, dtype=np.float64)\n",
    "                if batch_id not in self.outputs:\n",
    "                    self.outputs[batch_id] = [None] * self.neuron_count\n",
    "                    self.completed_count[batch_id] = 0\n",
    "                if self.outputs[batch_id][neuron_id] is None:\n",
    "                    self.outputs[batch_id][neuron_id] = output\n",
    "                    self.completed_count[batch_id] += 1\n",
    "                if self.completed_count[batch_id] == self.neuron_count:\n",
    "                    batch_size = int(batch_size)\n",
    "                    columns_size = int(columns_size)\n",
    "                    self.aggregate_neuron_outputs(batch_id, batch_size, columns_size)\n",
    "                    del self.outputs[batch_id]\n",
    "                    del self.completed_count[batch_id]\n",
    "            if not got_message and (time.time() - last_msg_time > 10):\n",
    "                consumer.commit()\n",
    "                consumer.close()\n",
    "                self.producer.close()\n",
    "                break\n",
    "            time.sleep(0.05)\n",
    "\n",
    "    def aggregate_neuron_outputs(self, batch_id, batch_size, columns_size):\n",
    "        #outputs = self.redis_handler.get_batch_multi(self.batch_keys[batch_id], batch_size)\n",
    "        outputs = np.squeeze(np.stack(self.outputs[batch_id], axis=1))\n",
    "        # Store the aggregated result in Redis.\n",
    "        if not self.is_final_layer:\n",
    "            self.redis_handler.set(f\"batch:{batch_id}:{self.layer_id}\", outputs, True, 1000)\n",
    "            self.activate_next_layer(batch_id, batch_size, columns_size)\n",
    "        else:\n",
    "            preds = np.argmax(outputs, axis=1)\n",
    "            cnt = batch_id * int(batch_size)\n",
    "            mapping = {idx + cnt: int(prediction) for idx, prediction in enumerate(preds)}\n",
    "            self.redis_handler.hset_bulk(\"batch:predictions\", mapping)\n",
    "            self.redis_handler.delete_batch_keys(batch_id)\n",
    "\n",
    "    def activate_next_layer(self, batch_id, batch_size, columns_size):\n",
    "        next_layer = f'layer_{self.layer_id_num + 1}'\n",
    "        self.producer.send('activate-layer', f\"{next_layer}|{batch_id}|{batch_size}|{self.neuron_count}\", self.layer_id_num + 1)\n",
    "        self.producer.send(f'requests-responses', 'www.layercoordinator.example')\n",
    "            \n",
    "\n",
    "class Layer(threading.Thread):\n",
    "    def __init__(self, layer_id, neuron_count):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.layer_id = layer_id\n",
    "        self.neuron_count = neuron_count\n",
    "        self.layer_id_num = int(self.layer_id.replace(\"layer_\", \"\"))\n",
    "        self.executor = ThreadPoolExecutor(max_workers=8)\n",
    "        self.producer = None\n",
    "\n",
    "    def activate_neurons(self, batch_id, batch_size, columns_size):\n",
    "        for neuron_id in range(self.neuron_count):\n",
    "            self.executor.submit(self.send_activation, neuron_id, batch_id, batch_size, columns_size)\n",
    "\n",
    "    def send_activation(self, neuron_id, batch_id, batch_size, columns_size):\n",
    "        self.producer.send(f'layer-{self.layer_id_num}', f\"{self.layer_id}|{batch_id}|{batch_size}|{columns_size}\", neuron_id)\n",
    "        #self.producer.send(f'requests-responses', 'www.layer.example')\n",
    "\n",
    "    def run(self):\n",
    "        consumer = KafkaConsumerHandler('activate-layer', KAFKA_BROKER, self.layer_id_num)\n",
    "        self.producer = KafkaProducerHandler(KAFKA_BROKER)\n",
    "        last_msg_time = time.time()\n",
    "        while True:\n",
    "            got_message = False\n",
    "            for message in consumer.consume():\n",
    "                got_message = True\n",
    "                last_msg_time = time.time()\n",
    "                layer, batch_id_str, batch_size, columns_size = message.split('|')\n",
    "                if layer == self.layer_id:\n",
    "                    batch_id = int(batch_id_str)\n",
    "                    self.activate_neurons(batch_id, batch_size, columns_size)\n",
    "            if not got_message and (time.time() - last_msg_time > 10):\n",
    "                consumer.commit()\n",
    "                consumer.close()\n",
    "                self.producer.close()\n",
    "                break\n",
    "            time.sleep(0.05)\n",
    "\n",
    "def predict_data():\n",
    "    batch_size = 100\n",
    "    producer = KafkaProducerHandler(KAFKA_BROKER)\n",
    "    redis_handler = RedisHandler('host.docker.internal', 6379, 0)\n",
    "    s3_client = S3Client(\"host.docker.internal:9000\", \"admin\", \"admin123\")\n",
    "    buffer = s3_client.download_fileobj(\"my-bucket\", \"mnist.csv\")\n",
    "    content_str = buffer.getvalue().decode(\"utf-8\")\n",
    "    data = np.genfromtxt(io.StringIO(content_str), delimiter=',', skip_header=1)\n",
    "    features = data[:, :-1][:500]\n",
    "    for idx, batch in enumerate(batch_generator(features, batch_size), start=0):\n",
    "        redis_handler.set(f\"batch:{idx}:initial_data\", batch, True, 1000)\n",
    "        producer.send('activate-layer', f\"layer_0|{idx}|{batch_size}|784\", 0)\n",
    "    producer.close()\n",
    "\n",
    "# Load network and dataset\n",
    "data = json.load(open(\"node_based_model.json\"))\n",
    "#df = pd.read_csv('data/mnist.csv').head(10)\n",
    "\n",
    "neurons = []\n",
    "layers = []\n",
    "coordinators = []\n",
    "\n",
    "for layer_name, layer_info in data.items():\n",
    "    neurons += [Neuron(layer_id=layer_name, neuron_id=i, weights=node['weights'], bias=node['biases'], activation=node['activation'], is_final_layer=(layer_name == list(data.keys())[-1])) for i, node in enumerate(layer_info['nodes'])]\n",
    "    layers.append(Layer(layer_id=layer_name, neuron_count=len(layer_info['nodes'])))\n",
    "    coordinators.append(LayerCoordinator(layer_id=layer_name, neuron_count=len(layer_info['nodes']), is_final_layer=(layer_name == list(data.keys())[-1])))\n",
    "\n",
    "# Start all threads\n",
    "for thread in neurons + layers + coordinators:\n",
    "    thread.start()\n",
    "\n",
    "print(\"Threads started\")\n",
    "\n",
    "predict_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bd0459-be2d-4848-8dee-bcc2efdb85ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
