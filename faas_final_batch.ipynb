{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b4746c8-abd3-473a-b050-2580a2be4e21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import redis \n",
    "\n",
    "client = redis.Redis('host.docker.internal', 6379, 0)\n",
    "\n",
    "client.flushdb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e839f3a3-f827-4d85-b49e-d6fc3df00357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import threading\n",
    "import time\n",
    "import io\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pcomp.kafka_confluent_utils import KafkaProducerHandler, KafkaConsumerHandler\n",
    "from pcomp.activation_functions import ACTIVATIONS, relu, softmax\n",
    "from pcomp.redis_utils import RedisHandler\n",
    "from pcomp.minio_utils import MinioClient\n",
    "from pcomp.utils import batch_generator\n",
    "from pcomp import neuroncalc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "910fbab6-325f-4103-b1f1-c0a1dc385513",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threads started\n"
     ]
    }
   ],
   "source": [
    "# Kafka Configuration\n",
    "KAFKA_BROKER = 'kafka:9092'\n",
    "\n",
    "class Neuron(threading.Thread):\n",
    "    def __init__(self, layer_id, neuron_id, weights, bias, activation, is_final_layer):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.layer_id = layer_id\n",
    "        self.layer_id_num = int(self.layer_id.replace(\"layer_\", \"\"))\n",
    "        self.neuron_id = neuron_id\n",
    "        self.weights = np.array(weights)\n",
    "        self.bias = np.array(bias)\n",
    "        self.activation = activation\n",
    "        self.activation_func = ACTIVATIONS.get(activation, relu)\n",
    "        self.is_final_layer = is_final_layer\n",
    "        self.redis_handler = RedisHandler('host.docker.internal', 6379, 0)\n",
    "        self.executor = ThreadPoolExecutor(max_workers=4)\n",
    "        self.producer = None\n",
    "\n",
    "    def fetch_input(self, batch_id, batch_size, columns_size):\n",
    "        key = f\"batch:{batch_id}:initial_data\" if self.layer_id == 'layer_0' else f\"batch:{batch_id}:layer_{int(self.layer_id[-1]) - 1}\"\n",
    "        # Poll Redis until the data is available.\n",
    "        while True:\n",
    "            data = np.frombuffer(self.redis_handler.get(key), dtype=np.float64).reshape(-1, int(columns_size))\n",
    "            if data is not None:\n",
    "                return data\n",
    "            print(f\"â³ Neuron {self.neuron_id} waiting for input data for key: {key}\")\n",
    "\n",
    "    def process_and_send(self, batch_id, batch_size, columns_size):\n",
    "        input_data = self.fetch_input(batch_id, batch_size, columns_size)\n",
    "        # z = np.dot(input_data, self.weights) + self.bias\n",
    "        # output = z if self.is_final_layer else self.activation_func(z)\n",
    "        activation_fn = \"None\" if self.is_final_layer else self.activation\n",
    "        output = neuroncalc.compute_neuron_output_batch(\n",
    "            input_data, self.weights, self.bias,\n",
    "            activation_fn\n",
    "        )\n",
    "        self.redis_handler.set(f\"batch:{batch_id}:n_{self.layer_id_num}_{self.neuron_id}\", output, True, 1000)\n",
    "        msg = f\"{self.neuron_id}|{batch_id}|{batch_size}|{columns_size}\"\n",
    "        self.producer.send(f'layer-{self.layer_id_num}-complete', msg, self.layer_id_num)\n",
    "        #self.producer.send(f'requests-responses', 'www.neuron.example')\n",
    "\n",
    "    def run(self):\n",
    "        # Instantiate Kafka consumer and producer inside the thread.\n",
    "        consumer = KafkaConsumerHandler(f'layer-{self.layer_id_num}', KAFKA_BROKER, partition=self.neuron_id)\n",
    "        self.producer = KafkaProducerHandler(KAFKA_BROKER)\n",
    "        last_msg_time = time.time()\n",
    "        while True:\n",
    "            got_message = False\n",
    "            for message in consumer.consume():\n",
    "                got_message = True\n",
    "                last_msg_time = time.time()\n",
    "                layer, batch_id_str, batch_size, columns_size = message.split('|')\n",
    "                if layer == self.layer_id:\n",
    "                    batch_id = int(batch_id_str)\n",
    "                    self.executor.submit(self.process_and_send, batch_id, batch_size, columns_size)\n",
    "            if not got_message and (time.time() - last_msg_time > 10):\n",
    "                consumer.commit()\n",
    "                consumer.close()\n",
    "                self.producer.close()\n",
    "                break\n",
    "            time.sleep(0.05)\n",
    "\n",
    "\n",
    "class LayerCoordinator(threading.Thread):\n",
    "    def __init__(self, layer_id, neuron_count, is_final_layer=False):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.layer_id = layer_id\n",
    "        self.layer_id_num = int(self.layer_id.replace(\"layer_\", \"\"))\n",
    "        self.neuron_count = neuron_count\n",
    "        self.is_final_layer = is_final_layer\n",
    "        self.completed_neurons = {}\n",
    "        self.redis_handler = RedisHandler('host.docker.internal', 6379, 0)\n",
    "        self.executor = ThreadPoolExecutor(max_workers=4)\n",
    "        self.producer = None\n",
    "\n",
    "    def run(self):\n",
    "        consumer = KafkaConsumerHandler(f'layer-{self.layer_id_num}-complete', KAFKA_BROKER, self.layer_id_num)\n",
    "        self.producer = KafkaProducerHandler(KAFKA_BROKER)\n",
    "        last_msg_time = time.time()\n",
    "        while True:\n",
    "            got_message = False\n",
    "            for message in consumer.consume():\n",
    "                got_message = True\n",
    "                last_msg_time = time.time()\n",
    "                neuron_id, batch_id, batch_size, columns_size = message.split('|')\n",
    "                neuron_id = int(neuron_id)\n",
    "                batch_id = int(batch_id)\n",
    "                if batch_id not in self.completed_neurons:\n",
    "                    self.completed_neurons[batch_id] = set()\n",
    "                self.completed_neurons[batch_id].add(neuron_id)\n",
    "\n",
    "                if len(self.completed_neurons[batch_id]) == self.neuron_count:\n",
    "                    batch_size = int(batch_size)\n",
    "                    columns_size = int(columns_size)\n",
    "                    self.aggregate_neuron_outputs(batch_id, batch_size, columns_size)\n",
    "                    #self.executor.submit(self.aggregate_neuron_outputs, batch_id, batch_size, columns_size)\n",
    "                    del self.completed_neurons[batch_id]\n",
    "            if not got_message and (time.time() - last_msg_time > 10):\n",
    "                consumer.commit()\n",
    "                consumer.close()\n",
    "                self.producer.close()\n",
    "                break\n",
    "            time.sleep(0.05)\n",
    "\n",
    "    def aggregate_neuron_outputs(self, batch_id, batch_size, columns_size):\n",
    "        keys = [f\"batch:{batch_id}:n_{self.layer_id_num}_{neuron}\" for neuron in range(self.neuron_count)]\n",
    "        outputs = self.redis_handler.get_batch_multi(keys, batch_size)\n",
    "        #outputs = np.squeeze(np.stack([self.redis_handler.get_batch(f\"batch:{batch_id}:n_{self.layer_id_num}_{neuron}\", batch_size, 1) for neuron in range(self.neuron_count)], axis=1))\n",
    "        # Store the aggregated result in Redis.\n",
    "        if not self.is_final_layer:\n",
    "            self.redis_handler.set(f\"batch:{batch_id}:{self.layer_id}\", outputs, True, 1000)\n",
    "            self.activate_next_layer(batch_id, batch_size, columns_size)\n",
    "        else:\n",
    "            preds = np.argmax(outputs, axis=1)\n",
    "            cnt = batch_id * int(batch_size)\n",
    "            mapping = {idx + cnt: int(prediction) for idx, prediction in enumerate(preds)}\n",
    "            self.redis_handler.hset_bulk(\"batch:predictions\", mapping)\n",
    "            self.redis_handler.delete_batch_keys(batch_id)\n",
    "\n",
    "    def activate_next_layer(self, batch_id, batch_size, columns_size):\n",
    "        next_layer = f'layer_{self.layer_id_num + 1}'\n",
    "        self.producer.send('activate-layer', f\"{next_layer}|{batch_id}|{batch_size}|{self.neuron_count}\", self.layer_id_num + 1)\n",
    "        self.producer.send(f'requests-responses', 'www.layercoordinator.example')\n",
    "            \n",
    "\n",
    "class Layer(threading.Thread):\n",
    "    def __init__(self, layer_id, neuron_count):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.layer_id = layer_id\n",
    "        self.neuron_count = neuron_count\n",
    "        self.layer_id_num = int(self.layer_id.replace(\"layer_\", \"\"))\n",
    "        self.executor = ThreadPoolExecutor(max_workers=8)\n",
    "        self.producer = None\n",
    "\n",
    "    def activate_neurons(self, batch_id, batch_size, columns_size):\n",
    "        for neuron_id in range(self.neuron_count):\n",
    "            self.executor.submit(self.send_activation, neuron_id, batch_id, batch_size, columns_size)\n",
    "\n",
    "    def send_activation(self, neuron_id, batch_id, batch_size, columns_size):\n",
    "        self.producer.send(f'layer-{self.layer_id_num}', f\"{self.layer_id}|{batch_id}|{batch_size}|{columns_size}\", neuron_id)\n",
    "        #self.producer.send(f'requests-responses', 'www.layer.example')\n",
    "\n",
    "    def run(self):\n",
    "        consumer = KafkaConsumerHandler('activate-layer', KAFKA_BROKER, self.layer_id_num)\n",
    "        self.producer = KafkaProducerHandler(KAFKA_BROKER)\n",
    "        last_msg_time = time.time()\n",
    "        while True:\n",
    "            got_message = False\n",
    "            for message in consumer.consume():\n",
    "                got_message = True\n",
    "                last_msg_time = time.time()\n",
    "                layer, batch_id_str, batch_size, columns_size = message.split('|')\n",
    "                if layer == self.layer_id:\n",
    "                    batch_id = int(batch_id_str)\n",
    "                    self.activate_neurons(batch_id, batch_size, columns_size)\n",
    "            if not got_message and (time.time() - last_msg_time > 10):\n",
    "                consumer.commit()\n",
    "                consumer.close()\n",
    "                self.producer.close()\n",
    "                break\n",
    "            time.sleep(0.05)\n",
    "\n",
    "def predict_data():\n",
    "    batch_size = 100\n",
    "    producer = KafkaProducerHandler(KAFKA_BROKER)\n",
    "    redis_handler = RedisHandler('host.docker.internal', 6379, 0)\n",
    "    file = MinioClient(\"host.docker.internal:9000\", \"admin\", \"admin123\").get_object(\"my-bucket\", \"mnist.csv\")\n",
    "    data = np.genfromtxt(io.StringIO(file.read().decode('utf-8')), delimiter=',', skip_header=1)\n",
    "    features = data[:, :-1][:400]\n",
    "    for idx, batch in enumerate(batch_generator(features, batch_size), start=0):\n",
    "        redis_handler.set(f\"batch:{idx}:initial_data\", batch, True, 1000)\n",
    "        producer.send('activate-layer', f\"layer_0|{idx}|{batch_size}|784\", 0)\n",
    "    producer.close()\n",
    "\n",
    "# Load network and dataset\n",
    "data = json.load(open(\"node_based_model.json\"))\n",
    "#df = pd.read_csv('data/mnist.csv').head(10)\n",
    "\n",
    "neurons = []\n",
    "layers = []\n",
    "coordinators = []\n",
    "\n",
    "for layer_name, layer_info in data.items():\n",
    "    neurons += [Neuron(layer_id=layer_name, neuron_id=i, weights=node['weights'], bias=node['biases'], activation=node['activation'], is_final_layer=(layer_name == list(data.keys())[-1])) for i, node in enumerate(layer_info['nodes'])]\n",
    "    layers.append(Layer(layer_id=layer_name, neuron_count=len(layer_info['nodes'])))\n",
    "    coordinators.append(LayerCoordinator(layer_id=layer_name, neuron_count=len(layer_info['nodes']), is_final_layer=(layer_name == list(data.keys())[-1])))\n",
    "\n",
    "# Start all threads\n",
    "for thread in neurons + layers + coordinators:\n",
    "    thread.start()\n",
    "\n",
    "print(\"Threads started\")\n",
    "\n",
    "predict_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f922540-cc8f-4e58-af08-6d1fdd62fc58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
