{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b4746c8-abd3-473a-b050-2580a2be4e21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import redis \n",
    "\n",
    "client = redis.Redis('host.docker.internal', 6379, 0)\n",
    "\n",
    "client.flushdb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e839f3a3-f827-4d85-b49e-d6fc3df00357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import threading\n",
    "import time\n",
    "import io\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pcomp.kafka_handlers import KafkaProducerHandler, KafkaConsumerHandler, KafkaConsumerHandlerNeuron\n",
    "from pcomp.activation_functions import ACTIVATIONS, relu, softmax\n",
    "from pcomp.redis_utils import RedisHandler\n",
    "from pcomp.s3client import S3Client\n",
    "from pcomp.utils import batch_generator\n",
    "from pcomp import neuroncalc\n",
    "from pcomp.neurons_accumulator import NeuronsAccumulator\n",
    "from pcomp.avro_utils import avro_serialize_batch, avro_deserialize_batch\n",
    "from base64 import b64encode, b64decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "910fbab6-325f-4103-b1f1-c0a1dc385513",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threads started\n"
     ]
    }
   ],
   "source": [
    "# Kafka Configuration\n",
    "KAFKA_BROKER = 'kafka:9092'\n",
    "\n",
    "class Neuron(threading.Thread):\n",
    "    def __init__(self, layer_id, neuron_id, weights, bias, activation, is_final_layer):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.layer_id = layer_id\n",
    "        self.layer_id_num = int(self.layer_id.replace(\"layer_\", \"\"))\n",
    "        self.neuron_id = neuron_id\n",
    "        self.weights = np.array(weights)\n",
    "        self.bias = np.array(bias)\n",
    "        self.activation = activation\n",
    "        self.activation_func = ACTIVATIONS.get(activation, relu)\n",
    "        self.is_final_layer = is_final_layer\n",
    "        self.redis_handler = RedisHandler('host.docker.internal', 6379, 0)\n",
    "        self.executor = ThreadPoolExecutor(max_workers=4)\n",
    "        self.producer = None\n",
    "\n",
    "    def fetch_input(self, batch_id, batch_size, columns_size):\n",
    "        key = f\"batch:{batch_id}:initial_data\" if self.layer_id == 'layer_0' else f\"batch:{batch_id}:{int(self.layer_id[-1]) - 1}\"\n",
    "        # Poll Redis until the data is available.\n",
    "        while True:\n",
    "            data = np.frombuffer(self.redis_handler.get(key), dtype=np.float64).reshape(-1, int(columns_size))\n",
    "            if data is not None:\n",
    "                return data\n",
    "            print(f\"â³ Neuron {self.neuron_id} waiting for input data for key: {key}\")\n",
    "\n",
    "    def process_and_send(self, batch_id, batch_size, columns_size):\n",
    "        input_data = self.fetch_input(batch_id, batch_size, columns_size)\n",
    "        z = np.dot(input_data, self.weights) + self.bias\n",
    "        output = z if self.is_final_layer else self.activation_func(z)\n",
    "        # activation_fn = \"None\" if self.is_final_layer else self.activation\n",
    "        # output = neuroncalc.compute_neuron_output_batch(\n",
    "        #     input_data, self.weights, self.bias,\n",
    "        #     activation_fn\n",
    "        # )\n",
    "        #self.redis_handler.set(f\"batch:{batch_id}:n_{self.layer_id_num}_{self.neuron_id}\", output, True, 1000)\n",
    "        message_dict = {\n",
    "            \"neuron_id\": self.neuron_id,\n",
    "            \"batch_id\": batch_id,\n",
    "            \"batch_size\":int(batch_size),\n",
    "            \"columns_size\": int(columns_size),\n",
    "            \"data\": output.tobytes()\n",
    "        }\n",
    "        msg = avro_serialize_batch(message_dict)\n",
    "        self.producer.send_neuron(msg)\n",
    "        #self.producer.send(f'requests-responses', 'www.neuron.example')\n",
    "\n",
    "    def run(self):\n",
    "        # Instantiate Kafka consumer and producer inside the thread.\n",
    "        consumer = KafkaConsumerHandler(f'layer-{self.layer_id_num}', KAFKA_BROKER, group_id=f\"{self.neuron_id}_{self.layer_id_num}_group\")\n",
    "        self.producer = KafkaProducerHandler(KAFKA_BROKER, f'layer-{self.layer_id_num}-complete')\n",
    "        last_msg_time = time.time()\n",
    "        while True:\n",
    "            got_message = False\n",
    "            for message in consumer.consume():\n",
    "                got_message = True\n",
    "                last_msg_time = time.time()\n",
    "                layer, batch_id_str, batch_size, columns_size = message.split('|')\n",
    "                if layer == self.layer_id:\n",
    "                    batch_id = int(batch_id_str)\n",
    "                    self.process_and_send(batch_id, batch_size, columns_size)\n",
    "            if not got_message and (time.time() - last_msg_time > 10):\n",
    "                consumer.commit()\n",
    "                consumer.close()\n",
    "                self.producer.close()\n",
    "                break\n",
    "            time.sleep(0.05)\n",
    "\n",
    "\n",
    "class LayerCoordinator(threading.Thread):\n",
    "    def __init__(self, layer_id, neuron_count, is_final_layer=False):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.layer_id = layer_id\n",
    "        self.layer_id_num = int(self.layer_id.replace(\"layer_\", \"\"))\n",
    "        self.neuron_count = neuron_count\n",
    "        self.is_final_layer = is_final_layer\n",
    "        self.accumulators = {}\n",
    "        self.redis_handler = RedisHandler('host.docker.internal', 6379, 0)\n",
    "        self.executor = ThreadPoolExecutor(max_workers=4)\n",
    "        self.producer = None\n",
    "\n",
    "    def run(self):\n",
    "        consumer = KafkaConsumerHandlerNeuron(f'layer-{self.layer_id_num}-complete', KAFKA_BROKER, group_id=f\"{self.layer_id_num}_coord_group\")\n",
    "        self.producer = KafkaProducerHandler(KAFKA_BROKER, 'activate-layer')\n",
    "        last_msg_time = time.time()\n",
    "        while True:\n",
    "            got_message = False\n",
    "            for message in consumer.consume():\n",
    "                got_message = True\n",
    "                last_msg_time = time.time()\n",
    "                message_dict = avro_deserialize_batch(message.value())\n",
    "                neuron_id = message_dict[\"neuron_id\"]\n",
    "                batch_id = message_dict[\"batch_id\"]\n",
    "                data_bytes = message_dict[\"data\"]\n",
    "                output = np.frombuffer(data_bytes, dtype=np.float64)\n",
    "                try:\n",
    "                    acc = self.accumulators[batch_id]\n",
    "                except KeyError:\n",
    "                    acc = self.accumulators[batch_id] = NeuronsAccumulator(self.neuron_count)\n",
    "                if acc.outputs[neuron_id] is None:\n",
    "                    acc.outputs[neuron_id] = output\n",
    "                    acc.completed += 1\n",
    "                if acc.completed == self.neuron_count:\n",
    "                    batch_size = message_dict[\"batch_size\"]\n",
    "                    columns_size = message_dict[\"columns_size\"]\n",
    "                    self.aggregate_neuron_outputs(batch_id, batch_size, columns_size, acc.outputs)\n",
    "                    del self.accumulators[batch_id]\n",
    "            if not got_message and (time.time() - last_msg_time > 10):\n",
    "                consumer.commit()\n",
    "                consumer.close()\n",
    "                self.producer.close()\n",
    "                break\n",
    "            time.sleep(0.05)\n",
    "\n",
    "    def aggregate_neuron_outputs(self, batch_id, batch_size, columns_size, output_batch):\n",
    "        #outputs = self.redis_handler.get_batch_multi(self.batch_keys[batch_id], batch_size)\n",
    "        outputs = np.squeeze(np.stack(output_batch, axis=1))\n",
    "        # Store the aggregated result in Redis.\n",
    "        if not self.is_final_layer:\n",
    "            self.redis_handler.set(f\"batch:{batch_id}:{self.layer_id_num}\", outputs, True, 1000)\n",
    "            self.activate_next_layer(batch_id, batch_size, columns_size)\n",
    "        else:\n",
    "            preds = np.argmax(outputs, axis=1)\n",
    "            cnt = batch_id * int(batch_size)\n",
    "            mapping = {idx + cnt: int(prediction) for idx, prediction in enumerate(preds)}\n",
    "            self.redis_handler.hset_bulk(\"batch:predictions\", mapping)\n",
    "            self.redis_handler.delete_batch_keys(batch_id)\n",
    "\n",
    "    def activate_next_layer(self, batch_id, batch_size, columns_size):\n",
    "        next_layer = f'layer_{self.layer_id_num + 1}'\n",
    "        self.producer.send(f\"{next_layer}|{batch_id}|{batch_size}|{self.neuron_count}\")\n",
    "        #self.producer.send(f'requests-responses', 'www.layercoordinator.example')\n",
    "            \n",
    "\n",
    "class Layer(threading.Thread):\n",
    "    def __init__(self, layer_id, neuron_count):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.layer_id = layer_id\n",
    "        self.neuron_count = neuron_count\n",
    "        self.layer_id_num = int(self.layer_id.replace(\"layer_\", \"\"))\n",
    "        self.executor = ThreadPoolExecutor(max_workers=8)\n",
    "        self.producer = None\n",
    "\n",
    "    def activate_neurons(self, batch_id, batch_size, columns_size):\n",
    "        self.producer.send(f\"{self.layer_id}|{batch_id}|{batch_size}|{columns_size}\")\n",
    "\n",
    "    def run(self):\n",
    "        consumer = KafkaConsumerHandler('activate-layer', KAFKA_BROKER, group_id=f\"{self.layer_id_num}_group\")\n",
    "        self.producer = KafkaProducerHandler(KAFKA_BROKER, f'layer-{self.layer_id_num}')\n",
    "        last_msg_time = time.time()\n",
    "        while True:\n",
    "            got_message = False\n",
    "            for message in consumer.consume():\n",
    "                got_message = True\n",
    "                last_msg_time = time.time()\n",
    "                layer, batch_id_str, batch_size, columns_size = message.split('|')\n",
    "                if layer == self.layer_id:\n",
    "                    batch_id = int(batch_id_str)\n",
    "                    self.activate_neurons(batch_id, batch_size, columns_size)\n",
    "            if not got_message and (time.time() - last_msg_time > 10):\n",
    "                consumer.commit()\n",
    "                consumer.close()\n",
    "                self.producer.close()\n",
    "                break\n",
    "            time.sleep(0.05)\n",
    "\n",
    "def predict_data():\n",
    "    batch_size = 50\n",
    "    producer = KafkaProducerHandler(KAFKA_BROKER, 'activate-layer')\n",
    "    redis_handler = RedisHandler('host.docker.internal', 6379, 0)\n",
    "    s3_client = S3Client(\"host.docker.internal:9000\", \"admin\", \"admin123\")\n",
    "    buffer = s3_client.download_fileobj(\"my-bucket\", \"mnist.csv\")\n",
    "    content_str = buffer.getvalue().decode(\"utf-8\")\n",
    "    data = np.genfromtxt(io.StringIO(content_str), delimiter=',', skip_header=1)\n",
    "    features = data[:, :-1][:200]\n",
    "    for idx, batch in enumerate(batch_generator(features, batch_size), start=0):\n",
    "        redis_handler.set(f\"batch:{idx}:initial_data\", batch, True, 1000)\n",
    "        producer.send(f\"layer_0|{idx}|{batch_size}|784\")\n",
    "    producer.close()\n",
    "\n",
    "# Load network and dataset\n",
    "data = json.load(open(\"node_based_model.json\"))\n",
    "#df = pd.read_csv('data/mnist.csv').head(10)\n",
    "\n",
    "neurons = []\n",
    "layers = []\n",
    "coordinators = []\n",
    "\n",
    "for layer_name, layer_info in data.items():\n",
    "    neurons += [Neuron(layer_id=layer_name, neuron_id=i, weights=node['weights'], bias=node['biases'], activation=node['activation'], is_final_layer=(layer_name == list(data.keys())[-1])) for i, node in enumerate(layer_info['nodes'])]\n",
    "    layers.append(Layer(layer_id=layer_name, neuron_count=len(layer_info['nodes'])))\n",
    "    coordinators.append(LayerCoordinator(layer_id=layer_name, neuron_count=len(layer_info['nodes']), is_final_layer=(layer_name == list(data.keys())[-1])))\n",
    "\n",
    "# Start all threads\n",
    "for thread in neurons + layers + coordinators:\n",
    "    thread.start()\n",
    "\n",
    "print(\"Threads started\")\n",
    "\n",
    "predict_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0bd0459-be2d-4848-8dee-bcc2efdb85ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "for thread in neurons + layers + coordinators:\n",
    "    thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbf1d33-524b-4910-967b-919140712609",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
