{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc414404-4f95-4c9f-a86a-6a68cc629eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 Sending initial activation message to layer-0...\n",
      "📤 Layer layer_0 sending activation messages to topic layer-0...\n",
      "✅ Message sent to layer-0, partition 0\n",
      "✅ Message sent to layer-0, partition 1\n",
      "✅ Message sent to layer-0, partition 2\n",
      "✅ Message sent to layer-0, partition 3\n",
      "✅ Message sent to layer-0, partition 4\n",
      "✅ Message sent to layer-0, partition 5\n",
      "✅ Message sent to layer-0, partition 6\n",
      "✅ Message sent to layer-0, partition 7\n",
      "✅ Message sent to layer-0, partition 8\n",
      "✅ Message sent to layer-0, partition 9\n",
      "✅ Message sent to layer-0, partition 10\n",
      "✅ Message sent to layer-0, partition 11\n",
      "✅ Message sent to layer-0, partition 12\n",
      "✅ Message sent to layer-0, partition 13\n",
      "✅ Message sent to layer-0, partition 14\n",
      "✅ Message sent to layer-0, partition 15\n",
      "✅ Message sent to layer-0, partition 16\n",
      "✅ Message sent to layer-0, partition 17\n",
      "✅ Message sent to layer-0, partition 18\n",
      "✅ Message sent to layer-0, partition 19\n",
      "✅ Message sent to layer-0, partition 20\n",
      "✅ Message sent to layer-0, partition 21\n",
      "✅ Message sent to layer-0, partition 22\n",
      "✅ Message sent to layer-0, partition 23\n",
      "✅ Message sent to layer-0, partition 24\n",
      "✅ Message sent to layer-0, partition 25\n",
      "✅ Message sent to layer-0, partition 26\n",
      "✅ Message sent to layer-0, partition 27\n",
      "✅ Message sent to layer-0, partition 28\n",
      "✅ Message sent to layer-0, partition 29\n",
      "✅ Message sent to layer-0, partition 30\n",
      "✅ Message sent to layer-0, partition 31\n",
      "✅ Message sent to layer-0, partition 32\n",
      "✅ Message sent to layer-0, partition 33\n",
      "✅ Message sent to layer-0, partition 34\n",
      "✅ Message sent to layer-0, partition 35\n",
      "✅ Message sent to layer-0, partition 36\n",
      "✅ Message sent to layer-0, partition 37\n",
      "✅ Message sent to layer-0, partition 38\n",
      "✅ Message sent to layer-0, partition 39\n",
      "✅ Message sent to layer-0, partition 40\n",
      "✅ Message sent to layer-0, partition 41\n",
      "✅ Message sent to layer-0, partition 42\n",
      "✅ Message sent to layer-0, partition 43\n",
      "✅ Message sent to layer-0, partition 44\n",
      "✅ Message sent to layer-0, partition 45\n",
      "✅ Message sent to layer-0, partition 46\n",
      "✅ Message sent to layer-0, partition 47\n",
      "✅ Message sent to layer-0, partition 48\n",
      "✅ Message sent to layer-0, partition 49\n",
      "✅ Message sent to layer-0, partition 50\n",
      "✅ Message sent to layer-0, partition 51\n",
      "✅ Message sent to layer-0, partition 52\n",
      "✅ Message sent to layer-0, partition 53\n",
      "✅ Message sent to layer-0, partition 54\n",
      "✅ Message sent to layer-0, partition 55\n",
      "✅ Message sent to layer-0, partition 56\n",
      "✅ Message sent to layer-0, partition 57\n",
      "✅ Message sent to layer-0, partition 58\n",
      "✅ Message sent to layer-0, partition 59\n",
      "✅ Message sent to layer-0, partition 60\n",
      "✅ Message sent to layer-0, partition 61\n",
      "✅ Message sent to layer-0, partition 62\n",
      "✅ Message sent to layer-0, partition 63\n",
      "✅ Message sent to layer-0, partition 64\n",
      "✅ Message sent to layer-0, partition 65\n",
      "✅ Message sent to layer-0, partition 66\n",
      "✅ Message sent to layer-0, partition 67\n",
      "✅ Message sent to layer-0, partition 68\n",
      "✅ Message sent to layer-0, partition 69\n",
      "✅ Message sent to layer-0, partition 70\n",
      "✅ Message sent to layer-0, partition 71\n",
      "✅ Message sent to layer-0, partition 72\n",
      "✅ Message sent to layer-0, partition 73\n",
      "✅ Message sent to layer-0, partition 74\n",
      "✅ Message sent to layer-0, partition 75\n",
      "✅ Message sent to layer-0, partition 76\n",
      "✅ Message sent to layer-0, partition 77\n",
      "✅ Message sent to layer-0, partition 78\n",
      "✅ Message sent to layer-0, partition 79\n",
      "✅ Message sent to layer-0, partition 80\n",
      "✅ Message sent to layer-0, partition 81\n",
      "✅ Message sent to layer-0, partition 82\n",
      "✅ Message sent to layer-0, partition 83\n",
      "✅ Message sent to layer-0, partition 84\n",
      "✅ Message sent to layer-0, partition 85\n",
      "✅ Message sent to layer-0, partition 86\n",
      "✅ Message sent to layer-0, partition 87\n",
      "✅ Message sent to layer-0, partition 88\n",
      "✅ Message sent to layer-0, partition 89\n",
      "✅ Message sent to layer-0, partition 90\n",
      "✅ Message sent to layer-0, partition 91\n",
      "✅ Message sent to layer-0, partition 92\n",
      "✅ Message sent to layer-0, partition 93\n",
      "✅ Message sent to layer-0, partition 94\n",
      "✅ Message sent to layer-0, partition 95\n",
      "✅ Message sent to layer-0, partition 96\n",
      "✅ Message sent to layer-0, partition 97\n",
      "✅ Message sent to layer-0, partition 98\n",
      "✅ Message sent to layer-0, partition 99\n",
      "✅ Message sent to layer-0, partition 100\n",
      "✅ Message sent to layer-0, partition 101\n",
      "✅ Message sent to layer-0, partition 102\n",
      "✅ Message sent to layer-0, partition 103\n",
      "✅ Message sent to layer-0, partition 104\n",
      "✅ Message sent to layer-0, partition 105\n",
      "✅ Message sent to layer-0, partition 106\n",
      "✅ Message sent to layer-0, partition 107\n",
      "✅ Message sent to layer-0, partition 108\n",
      "✅ Message sent to layer-0, partition 109\n",
      "✅ Message sent to layer-0, partition 110\n",
      "✅ Message sent to layer-0, partition 111\n",
      "✅ Message sent to layer-0, partition 112\n",
      "✅ Message sent to layer-0, partition 113\n",
      "✅ Message sent to layer-0, partition 114\n",
      "✅ Message sent to layer-0, partition 115\n",
      "✅ Message sent to layer-0, partition 116\n",
      "✅ Message sent to layer-0, partition 117\n",
      "✅ Message sent to layer-0, partition 118\n",
      "✅ Message sent to layer-0, partition 119\n",
      "✅ Message sent to layer-0, partition 120\n",
      "✅ Message sent to layer-0, partition 121\n",
      "✅ Message sent to layer-0, partition 122\n",
      "✅ Message sent to layer-0, partition 123\n",
      "✅ Message sent to layer-0, partition 124\n",
      "✅ Message sent to layer-0, partition 125\n",
      "✅ Message sent to layer-0, partition 126\n",
      "✅ Message sent to layer-0, partition 127\n",
      "✅ Neuron 0 received message: {'layer': 'layer_0'}\n",
      "🚀 Neuron 0 in layer_0 activated!\n",
      "✅ Neuron 1 received message: {'layer': 'layer_0'}\n",
      "🚀 Neuron 1 in layer_0 activated!\n",
      "✅ Neuron 2 received message: {'layer': 'layer_0'}\n",
      "🚀 Neuron 2 in layer_0 activated!\n",
      "✅ Neuron 3 received message: {'layer': 'layer_0'}\n",
      "🚀 Neuron 3 in layer_0 activated!\n",
      "✅ Neuron 4 received message: {'layer': 'layer_0'}\n",
      "🚀 Neuron 4 in layer_0 activated!\n",
      "✅ Neuron 5 received message: {'layer': 'layer_0'}\n",
      "🚀 Neuron 5 in layer_0 activated!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 158\u001b[0m\n\u001b[1;32m    156\u001b[0m     image, label \u001b[38;5;241m=\u001b[39m mnist_test[i]\n\u001b[1;32m    157\u001b[0m     image_np \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m--> 158\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mforward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Prediction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprediction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Label: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# Calculate accuracy\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 137\u001b[0m, in \u001b[0;36mforward_pass\u001b[0;34m(layers, input_data, image_id)\u001b[0m\n\u001b[1;32m    134\u001b[0m producer\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m layers:\n\u001b[0;32m--> 137\u001b[0m     input_data \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39margmax(input_data))\n\u001b[1;32m    140\u001b[0m redis_client\u001b[38;5;241m.\u001b[39mhset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m'\u001b[39m, image_id, prediction)  \u001b[38;5;66;03m# Store all predictions\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 87\u001b[0m, in \u001b[0;36mLayer.forward\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m     84\u001b[0m producer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m     85\u001b[0m producer\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m---> 87\u001b[0m outputs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([neuron\u001b[38;5;241m.\u001b[39mforward(input_data) \u001b[38;5;28;01mfor\u001b[39;00m neuron \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneurons])\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_final_layer:\n\u001b[1;32m     89\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m softmax(outputs)\n",
      "Cell \u001b[0;32mIn[8], line 87\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     84\u001b[0m producer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m     85\u001b[0m producer\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m---> 87\u001b[0m outputs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mneuron\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m neuron \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneurons])\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_final_layer:\n\u001b[1;32m     89\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m softmax(outputs)\n",
      "Cell \u001b[0;32mIn[8], line 52\u001b[0m, in \u001b[0;36mNeuron.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     49\u001b[0m partition \u001b[38;5;241m=\u001b[39m TopicPartition(topic, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneuron_id)\n\u001b[1;32m     50\u001b[0m consumer\u001b[38;5;241m.\u001b[39massign([partition])\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m consumer:\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Neuron \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneuron_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m received message: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m message\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mand\u001b[39;00m message\u001b[38;5;241m.\u001b[39mvalue[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_id:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/kafka/consumer/group.py:1197\u001b[0m, in \u001b[0;36mKafkaConsumer.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_v1()\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/kafka/consumer/group.py:1205\u001b[0m, in \u001b[0;36mKafkaConsumer.next_v2\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_message_generator_v2()\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   1207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/kafka/consumer/group.py:1120\u001b[0m, in \u001b[0;36mKafkaConsumer._message_generator_v2\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_message_generator_v2\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1119\u001b[0m     timeout_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consumer_timeout \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mtime())\n\u001b[0;32m-> 1120\u001b[0m     record_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout_ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_offsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tp, records \u001b[38;5;129;01min\u001b[39;00m six\u001b[38;5;241m.\u001b[39miteritems(record_map):\n\u001b[1;32m   1122\u001b[0m         \u001b[38;5;66;03m# Generators are stateful, and it is possible that the tp / records\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m         \u001b[38;5;66;03m# here may become stale during iteration -- i.e., we seek to a\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m         \u001b[38;5;66;03m# different offset, pause consumption, or lose assignment.\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m records:\n\u001b[1;32m   1126\u001b[0m             \u001b[38;5;66;03m# is_fetchable(tp) should handle assignment changes and offset\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m             \u001b[38;5;66;03m# resets; for all other changes (e.g., seeks) we'll rely on the\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m             \u001b[38;5;66;03m# outer function destroying the existing iterator/generator\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m             \u001b[38;5;66;03m# via self._iterator = None\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/kafka/consumer/group.py:657\u001b[0m, in \u001b[0;36mKafkaConsumer.poll\u001b[0;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[1;32m    655\u001b[0m remaining \u001b[38;5;241m=\u001b[39m timeout_ms\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_closed:\n\u001b[0;32m--> 657\u001b[0m     records \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_records\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_offsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupdate_offsets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m records:\n\u001b[1;32m    659\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m records\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/kafka/consumer/group.py:706\u001b[0m, in \u001b[0;36mKafkaConsumer._poll_once\u001b[0;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mpoll(timeout_ms\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    705\u001b[0m timeout_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(timeout_ms, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coordinator\u001b[38;5;241m.\u001b[39mtime_to_next_poll() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m--> 706\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout_ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;66;03m# after the long poll, we should check whether the group needs to rebalance\u001b[39;00m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;66;03m# prior to returning data so that the group can stabilize faster\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coordinator\u001b[38;5;241m.\u001b[39mneed_rejoin():\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/kafka/client_async.py:614\u001b[0m, in \u001b[0;36mKafkaClient.poll\u001b[0;34m(self, timeout_ms, future)\u001b[0m\n\u001b[1;32m    607\u001b[0m         timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\n\u001b[1;32m    608\u001b[0m             timeout_ms,\n\u001b[1;32m    609\u001b[0m             metadata_timeout_ms,\n\u001b[1;32m    610\u001b[0m             idle_connection_timeout_ms,\n\u001b[1;32m    611\u001b[0m             request_timeout_ms)\n\u001b[1;32m    612\u001b[0m         timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, timeout)  \u001b[38;5;66;03m# avoid negative timeouts\u001b[39;00m\n\u001b[0;32m--> 614\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;66;03m# called without the lock to avoid deadlock potential\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;66;03m# if handlers need to acquire locks\u001b[39;00m\n\u001b[1;32m    618\u001b[0m responses\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fire_pending_completed_requests())\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/kafka/client_async.py:648\u001b[0m, in \u001b[0;36mKafkaClient._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_register_send_sockets()\n\u001b[1;32m    647\u001b[0m start_select \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 648\u001b[0m ready \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    649\u001b[0m end_select \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sensors:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/selectors.py:469\u001b[0m, in \u001b[0;36mEpollSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    467\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 469\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ev\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from kafka import KafkaProducer, KafkaConsumer, TopicPartition\n",
    "import redis\n",
    "import time\n",
    "\n",
    "\n",
    "offset = \"latest\"\n",
    "\n",
    "\n",
    "# Redis setup\n",
    "redis_client = redis.Redis(host='host.docker.internal', port=6379, db=0)\n",
    "\n",
    "# Activation functions\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))  # Stability trick\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "ACTIVATIONS = {\n",
    "    \"relu\": relu,\n",
    "    \"softmax\": softmax\n",
    "}\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, layer_id, neuron_id, weights, bias, activation, is_final_layer=False):\n",
    "        self.layer_id = layer_id\n",
    "        self.neuron_id = neuron_id\n",
    "        self.weights = np.array(weights)\n",
    "        self.bias = np.array(bias)\n",
    "        self.activation_func = None if is_final_layer else ACTIVATIONS.get(activation, relu)\n",
    "        self.is_final_layer = is_final_layer\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Wait for activation message from Kafka before processing\"\"\"\n",
    "        topic = f'layer-{self.layer_id[-1]}'\n",
    "        consumer = KafkaConsumer(\n",
    "            bootstrap_servers=KAFKA_BROKER,\n",
    "            value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
    "            auto_offset_reset=OFFSET_RESET,\n",
    "            enable_auto_commit=True,\n",
    "            group_id=f'group-layer-{self.layer_id}',\n",
    "            consumer_timeout_ms=120000  # Increased timeout to avoid early exit\n",
    "        )\n",
    "\n",
    "        partition = TopicPartition(topic, self.neuron_id)\n",
    "        consumer.assign([partition])\n",
    "\n",
    "        print(f\"Neuron {self.neuron_id} in {self.layer_id} waiting for activation...\")\n",
    "\n",
    "        while True:  # Keep waiting until message is received\n",
    "            for message in consumer:\n",
    "                print(f\"Neuron {self.neuron_id} received message: {message.value}\")\n",
    "                if 'layer' in message.value and message.value['layer'] == self.layer_id:\n",
    "                    print(f\"Neuron {self.neuron_id} in {self.layer_id} activated!\")\n",
    "                    consumer.close()\n",
    "                    z = np.dot(inputs, self.weights) + self.bias\n",
    "                    return z if self.is_final_layer else self.activation_func(z)\n",
    "            \n",
    "            print(f\"⚠️ Neuron {self.neuron_id} in {self.layer_id} still waiting for activation...\")\n",
    "            time.sleep(2)  # Prevents infinite loop from consuming CPU\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, layer_id, neurons, is_final_layer=False):\n",
    "        self.layer_id = layer_id\n",
    "        self.neurons = neurons\n",
    "        self.is_final_layer = is_final_layer\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        \"\"\"Trigger neuron activations via Kafka and activate the next layer when finished\"\"\"\n",
    "        producer = KafkaProducer(bootstrap_servers='kafka:9092',\n",
    "                                 value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
    "                                 retries=5, request_timeout_ms=10000)\n",
    "    \n",
    "        topic = f'layer-{self.layer_id[-1]}'\n",
    "        activation_message = {'layer': self.layer_id}\n",
    "\n",
    "        print(f\"📤 Layer {self.layer_id} sending activation messages to topic {topic}...\")\n",
    "\n",
    "        for neuron_id in range(len(self.neurons)):\n",
    "            producer.send(topic, key=str(neuron_id).encode(), value=activation_message, partition=neuron_id)\n",
    "            print(f\"✅ Message sent to {topic}, partition {neuron_id}\")\n",
    "\n",
    "        producer.flush()\n",
    "        producer.close()\n",
    "\n",
    "        outputs = np.array([neuron.forward(input_data) for neuron in self.neurons])\n",
    "        if self.is_final_layer:\n",
    "            outputs = softmax(outputs)\n",
    "        \n",
    "        redis_client.set(self.layer_id, outputs.astype(np.float32).tobytes())\n",
    "        \n",
    "        # Activate the next layer\n",
    "        producer = KafkaProducer(bootstrap_servers='kafka:9092',\n",
    "                                 value_serializer=lambda v: json.dumps(v).encode('utf-8'))\n",
    "        producer.send('activate-layer', {'layer': f'layer-{int(self.layer_id[-1]) + 1}'} if not self.is_final_layer else {'layer': 'final'})\n",
    "        producer.flush()\n",
    "        producer.close()\n",
    "    \n",
    "        return outputs\n",
    "\n",
    "# Load JSON file\n",
    "def load_network(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Build network\n",
    "def build_network(json_data):\n",
    "    layers = []\n",
    "    sorted_layers = sorted(json_data.keys(), key=lambda x: int(x.split('_')[-1]))\n",
    "    for i, layer_name in enumerate(sorted_layers):\n",
    "        layer_info = json_data[layer_name]\n",
    "        neurons = [\n",
    "            Neuron(\n",
    "                layer_id=layer_name,\n",
    "                neuron_id=idx,\n",
    "                weights=np.array(node['weights']),\n",
    "                bias=np.array(node['biases']),\n",
    "                activation=node['activation'],\n",
    "                is_final_layer=(i == len(sorted_layers) - 1)\n",
    "            )\n",
    "            for idx, node in enumerate(layer_info['nodes'])\n",
    "        ]\n",
    "        layers.append(Layer(layer_id=layer_name, neurons=neurons, is_final_layer=(i == len(sorted_layers) - 1)))\n",
    "    return layers\n",
    "\n",
    "# Forward pass for single image\n",
    "def forward_pass(layers, input_data, image_id):\n",
    "    producer = KafkaProducer(bootstrap_servers='kafka:9092',\n",
    "                             value_serializer=lambda v: json.dumps(v).encode('utf-8'))\n",
    "    print(\"🔥 Sending initial activation message to layer-0...\")\n",
    "    producer.send('layer-0', {'layer': 'layer-0'})\n",
    "    producer.flush()\n",
    "    producer.close()\n",
    "    \n",
    "    for layer in layers:\n",
    "        input_data = layer.forward(input_data)\n",
    "    \n",
    "    prediction = int(np.argmax(input_data))\n",
    "    redis_client.hset('predictions', image_id, prediction)  # Store all predictions\n",
    "    return prediction\n",
    "\n",
    "# Load network\n",
    "data = load_network(\"node_based_model.json\")\n",
    "network = build_network(data)\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "mnist_test = datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "# Test first 10 images one by one\n",
    "for i in range(10):\n",
    "    image, label = mnist_test[i]\n",
    "    image_np = image.view(-1).numpy()\n",
    "    prediction = forward_pass(network, image_np, i)\n",
    "    print(f\"Image {i} Prediction: {prediction}, Label: {label}\")\n",
    "\n",
    "# Calculate accuracy\n",
    "predictions = redis_client.hgetall('predictions')\n",
    "correct = sum(int(predictions[k]) == mnist_test[int(k)][1] for k in predictions)\n",
    "accuracy = correct / len(predictions)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4882dd09-14f3-49a8-ac46-0df78f94433d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
