{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13c183cc-ea78-429b-b9f3-dcde1b473b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 0: Label = 7, Predicted = 7\n",
      "Image 1: Label = 2, Predicted = 2\n",
      "Image 2: Label = 1, Predicted = 1\n",
      "Image 3: Label = 0, Predicted = 0\n",
      "Image 4: Label = 4, Predicted = 4\n",
      "Image 5: Label = 1, Predicted = 1\n",
      "Image 6: Label = 4, Predicted = 4\n",
      "Image 7: Label = 9, Predicted = 9\n",
      "Image 8: Label = 5, Predicted = 6\n",
      "Image 9: Label = 9, Predicted = 9\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Load JSON file\n",
    "def load_network(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Activation functions\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # Stability trick\n",
    "    return e_x / e_x.sum(axis=1, keepdims=True)\n",
    "\n",
    "ACTIVATIONS = {\n",
    "    \"relu\": relu,\n",
    "    \"softmax\": softmax\n",
    "}\n",
    "\n",
    "# Global storage (simulating Redis)\n",
    "storage = {}\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, weights, bias, activation, layer_id, neuron_id, is_final_layer=False):\n",
    "        self.weights = np.array(weights)\n",
    "        self.bias = np.array(bias)\n",
    "        self.activation_func = None if is_final_layer else ACTIVATIONS.get(activation, relu)\n",
    "        self.layer_id = layer_id\n",
    "        self.neuron_id = neuron_id\n",
    "        self.is_final_layer = is_final_layer\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Ensure inputs match expected shape\n",
    "        input_values = np.array([storage[inp] for inp in inputs if inp in storage])  # shape (batch_size, num_inputs)\n",
    "        input_values = np.stack(input_values, axis=1)  # Reshape properly\n",
    "\n",
    "        if input_values.shape[1] != self.weights.shape[0]:\n",
    "            raise ValueError(f\"Neuron {self.layer_id}_{self.neuron_id} received {input_values.shape[1]} inputs \"\n",
    "                             f\"but expected {self.weights.shape[0]}\")\n",
    "\n",
    "        # Compute activation\n",
    "        z = np.dot(input_values, self.weights) + self.bias  # shape (batch_size,)\n",
    "        output = z if self.is_final_layer else self.activation_func(z)\n",
    "\n",
    "        # Store output\n",
    "        storage[f\"{self.layer_id}_{self.neuron_id}\"] = output\n",
    "        return output\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, layer_id, neurons):\n",
    "        self.layer_id = layer_id\n",
    "        self.neurons = neurons\n",
    "\n",
    "    def forward(self, input_keys):\n",
    "        inputs = [key for key in input_keys if key in storage]  # Ensure valid keys\n",
    "        outputs = [neuron.forward(inputs) for neuron in self.neurons]\n",
    "\n",
    "        return outputs\n",
    "\n",
    "# Build network dynamically from JSON\n",
    "def build_network(json_data):\n",
    "    layers = []\n",
    "    sorted_layers = sorted(json_data.keys(), key=lambda x: int(x.split('_')[-1]))  # Ensure correct order\n",
    "\n",
    "    for i, layer_name in enumerate(sorted_layers):\n",
    "        layer_info = json_data[layer_name]\n",
    "        layer_neurons = []\n",
    "\n",
    "        for j, node in enumerate(layer_info['nodes']):\n",
    "            neuron = Neuron(\n",
    "                weights=np.array(node['weights']),\n",
    "                bias=np.array(node['biases']),\n",
    "                activation=node['activation'],\n",
    "                layer_id=layer_name,\n",
    "                neuron_id=j,\n",
    "                is_final_layer=(i == len(sorted_layers) - 1)\n",
    "            )\n",
    "            layer_neurons.append(neuron)\n",
    "\n",
    "        layers.append(Layer(layer_name, layer_neurons))\n",
    "\n",
    "    return layers\n",
    "\n",
    "# Forward pass for batch processing\n",
    "def forward_pass(layers, input_data):\n",
    "    batch_size = input_data.shape[0]\n",
    "    storage.clear()  # Reset storage\n",
    "\n",
    "    # Initialize input storage for the batch\n",
    "    for i in range(input_data.shape[1]):  # Iterate over feature dimensions\n",
    "        storage[f\"input_{i}\"] = input_data[:, i]  # Store batch of input features\n",
    "\n",
    "    # Process each layer\n",
    "    input_keys = [f\"input_{i}\" for i in range(input_data.shape[1])]\n",
    "\n",
    "    for layer in layers:\n",
    "        layer.forward(input_keys)\n",
    "        input_keys = [f\"{layer.layer_id}_{j}\" for j in range(len(layer.neurons))]\n",
    "\n",
    "    # Retrieve final layer outputs\n",
    "    final_outputs = np.array([storage[f\"{layers[-1].layer_id}_{j}\"] for j in range(len(layers[-1].neurons))]).T  # Transpose to (batch_size, num_classes)\n",
    "\n",
    "    # Apply softmax at the final layer\n",
    "    final_outputs = softmax(final_outputs)\n",
    "\n",
    "    return np.argmax(final_outputs, axis=1)  # Return predicted classes for batch\n",
    "\n",
    "# Load network\n",
    "data = load_network(\"node_based_model.json\")\n",
    "network = build_network(data)\n",
    "\n",
    "# Load MNIST dataset with correct normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "])\n",
    "mnist_test = datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "# Process images in batches\n",
    "batch_size = 10\n",
    "images, labels = [], []\n",
    "\n",
    "for i in range(batch_size):  # Collect batch\n",
    "    image, label = mnist_test[i]\n",
    "    images.append(image.view(-1).numpy())  # Flatten 28x28 image to 784 input vector\n",
    "    labels.append(label)\n",
    "\n",
    "images = np.stack(images)  # Convert list to array of shape (batch_size, 784)\n",
    "\n",
    "# Run batch prediction\n",
    "predicted_classes = forward_pass(network, images)\n",
    "\n",
    "# Print results\n",
    "for i, (true_label, pred_label) in enumerate(zip(labels, predicted_classes)):\n",
    "    print(f\"Image {i}: Label = {true_label}, Predicted = {pred_label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c167e97-827a-4af7-ac0c-bab7e3c9c324",
   "metadata": {},
   "source": [
    "#### All at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02eed3d2-9331-430c-a080-324dc5dcee12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on full MNIST test set: 96.73%\n",
      "Image 0: True Label = 7, Predicted = 7\n",
      "Image 1: True Label = 2, Predicted = 2\n",
      "Image 2: True Label = 1, Predicted = 1\n",
      "Image 3: True Label = 0, Predicted = 0\n",
      "Image 4: True Label = 4, Predicted = 4\n",
      "Image 5: True Label = 1, Predicted = 1\n",
      "Image 6: True Label = 4, Predicted = 4\n",
      "Image 7: True Label = 9, Predicted = 9\n",
      "Image 8: True Label = 5, Predicted = 6\n",
      "Image 9: True Label = 9, Predicted = 9\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Load JSON file\n",
    "def load_network(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Activation functions\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # Stability trick\n",
    "    return e_x / e_x.sum(axis=1, keepdims=True)\n",
    "\n",
    "ACTIVATIONS = {\n",
    "    \"relu\": relu,\n",
    "    \"softmax\": softmax\n",
    "}\n",
    "\n",
    "# Global storage (simulating Redis)\n",
    "storage = {}\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, weights, bias, activation, layer_id, neuron_id, is_final_layer=False):\n",
    "        self.weights = np.array(weights)\n",
    "        self.bias = np.array(bias)\n",
    "        self.activation_func = None if is_final_layer else ACTIVATIONS.get(activation, relu)\n",
    "        self.layer_id = layer_id\n",
    "        self.neuron_id = neuron_id\n",
    "        self.is_final_layer = is_final_layer\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Ensure inputs match expected shape\n",
    "        input_values = np.array([storage[inp] for inp in inputs if inp in storage])  # shape (batch_size, num_inputs)\n",
    "        input_values = np.stack(input_values, axis=1)  # Reshape properly\n",
    "\n",
    "        if input_values.shape[1] != self.weights.shape[0]:\n",
    "            raise ValueError(f\"Neuron {self.layer_id}_{self.neuron_id} received {input_values.shape[1]} inputs \"\n",
    "                             f\"but expected {self.weights.shape[0]}\")\n",
    "\n",
    "        # Compute activation\n",
    "        z = np.dot(input_values, self.weights) + self.bias  # shape (batch_size,)\n",
    "        output = z if self.is_final_layer else self.activation_func(z)\n",
    "\n",
    "        # Store output\n",
    "        storage[f\"{self.layer_id}_{self.neuron_id}\"] = output\n",
    "        return output\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, layer_id, neurons):\n",
    "        self.layer_id = layer_id\n",
    "        self.neurons = neurons\n",
    "\n",
    "    def forward(self, input_keys):\n",
    "        inputs = [key for key in input_keys if key in storage]  # Ensure valid keys\n",
    "        outputs = [neuron.forward(inputs) for neuron in self.neurons]\n",
    "\n",
    "        return outputs\n",
    "\n",
    "# Build network dynamically from JSON\n",
    "def build_network(json_data):\n",
    "    layers = []\n",
    "    sorted_layers = sorted(json_data.keys(), key=lambda x: int(x.split('_')[-1]))  # Ensure correct order\n",
    "\n",
    "    for i, layer_name in enumerate(sorted_layers):\n",
    "        layer_info = json_data[layer_name]\n",
    "        layer_neurons = []\n",
    "\n",
    "        for j, node in enumerate(layer_info['nodes']):\n",
    "            neuron = Neuron(\n",
    "                weights=np.array(node['weights']),\n",
    "                bias=np.array(node['biases']),\n",
    "                activation=node['activation'],\n",
    "                layer_id=layer_name,\n",
    "                neuron_id=j,\n",
    "                is_final_layer=(i == len(sorted_layers) - 1)\n",
    "            )\n",
    "            layer_neurons.append(neuron)\n",
    "\n",
    "        layers.append(Layer(layer_name, layer_neurons))\n",
    "\n",
    "    return layers\n",
    "\n",
    "# Forward pass for full dataset\n",
    "def forward_pass(layers, input_data):\n",
    "    batch_size = input_data.shape[0]\n",
    "    storage.clear()  # Reset storage\n",
    "\n",
    "    # Initialize input storage for the entire dataset\n",
    "    for i in range(input_data.shape[1]):  # Iterate over feature dimensions\n",
    "        storage[f\"input_{i}\"] = input_data[:, i]  # Store batch of input features\n",
    "\n",
    "    # Process each layer\n",
    "    input_keys = [f\"input_{i}\" for i in range(input_data.shape[1])]\n",
    "\n",
    "    for layer in layers:\n",
    "        layer.forward(input_keys)\n",
    "        input_keys = [f\"{layer.layer_id}_{j}\" for j in range(len(layer.neurons))]\n",
    "\n",
    "    # Retrieve final layer outputs\n",
    "    final_outputs = np.array([storage[f\"{layers[-1].layer_id}_{j}\"] for j in range(len(layers[-1].neurons))]).T  # Transpose to (batch_size, num_classes)\n",
    "\n",
    "    # Apply softmax at the final layer\n",
    "    final_outputs = softmax(final_outputs)\n",
    "\n",
    "    return np.argmax(final_outputs, axis=1)  # Return predicted classes for full dataset\n",
    "\n",
    "# Load network\n",
    "data = load_network(\"node_based_model.json\")\n",
    "network = build_network(data)\n",
    "\n",
    "# Load entire MNIST test dataset with correct normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "])\n",
    "mnist_test = datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "# Convert full dataset to NumPy arrays\n",
    "images = np.stack([mnist_test[i][0].view(-1).numpy() for i in range(len(mnist_test))])  # (num_samples, 784)\n",
    "labels = np.array([mnist_test[i][1] for i in range(len(mnist_test))])  # (num_samples,)\n",
    "\n",
    "# Run prediction for entire dataset\n",
    "predicted_classes = forward_pass(network, images)\n",
    "\n",
    "# Evaluate Accuracy\n",
    "accuracy = np.mean(predicted_classes == labels)\n",
    "print(f\"Accuracy on full MNIST test set: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print some sample predictions\n",
    "for i in range(10):\n",
    "    print(f\"Image {i}: True Label = {labels[i]}, Predicted = {predicted_classes[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2096d79-54ea-4ec5-a314-8957e3ec17a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
